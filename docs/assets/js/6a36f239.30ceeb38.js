"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6167],{8453:(n,e,r)=>{r.d(e,{R:()=>i,x:()=>s});var t=r(6540);const a={},o=t.createContext(a);function i(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),t.createElement(o.Provider,{value:e},n.children)}},9978:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=r(4848),a=r(8453);const o={slug:"chapter-10-reinforcement-learning-control",title:"Chapter 10 - Reinforcement Learning for Control",description:"Comprehensive guide to reinforcement learning techniques for robotic control",tags:["reinforcement-learning","rl","control","robotics","ai","machine-learning"]},i="\ud83d\udcda Chapter 10: Reinforcement Learning for Control \ud83d\udcda",s={id:"part-4-ai-brain/chapter-10-vision-language-action",title:"Chapter 10 - Reinforcement Learning for Control",description:"Comprehensive guide to reinforcement learning techniques for robotic control",source:"@site/docusaurus/docs/part-4-ai-brain/chapter-10-vision-language-action.md",sourceDirName:"part-4-ai-brain",slug:"/part-4-ai-brain/chapter-10-reinforcement-learning-control",permalink:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-10-reinforcement-learning-control",draft:!1,unlisted:!1,editUrl:"https://github.com/aamna847/Humanoid-Robotic-Book/edit/main/docusaurus/docs/part-4-ai-brain/chapter-10-vision-language-action.md",tags:[{label:"reinforcement-learning",permalink:"/Humanoid-Robotic-Book/docs/tags/reinforcement-learning"},{label:"rl",permalink:"/Humanoid-Robotic-Book/docs/tags/rl"},{label:"control",permalink:"/Humanoid-Robotic-Book/docs/tags/control"},{label:"robotics",permalink:"/Humanoid-Robotic-Book/docs/tags/robotics"},{label:"ai",permalink:"/Humanoid-Robotic-Book/docs/tags/ai"},{label:"machine-learning",permalink:"/Humanoid-Robotic-Book/docs/tags/machine-learning"}],version:"current",frontMatter:{slug:"chapter-10-reinforcement-learning-control",title:"Chapter 10 - Reinforcement Learning for Control",description:"Comprehensive guide to reinforcement learning techniques for robotic control",tags:["reinforcement-learning","rl","control","robotics","ai","machine-learning"]},sidebar:"tutorialSidebar",previous:{title:"Chapter 9 - Visual SLAM & Navigation (Nav2)",permalink:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-9-visual-slam-navigation"},next:{title:"Chapter 11 - Humanoid Kinematics & Bipedal Locomotion",permalink:"/Humanoid-Robotic-Book/docs/part-5-advanced-humanoids/chapter-11-humanoid-kinematics-locomotion"}},c={},l=[{value:"\ud83c\udfaf Learning Objectives \ud83c\udfaf",id:"-learning-objectives-",level:2},{value:"\ud83d\udc4b 10.1 Introduction to Vision-Language-Action Integration \ud83d\udc4b",id:"-101-introduction-to-vision-language-action-integration-",level:2},{value:"\u2139\ufe0f 10.1.1 The VLA Framework \u2139\ufe0f",id:"\u2139\ufe0f-1011-the-vla-framework-\u2139\ufe0f",level:3},{value:"\ud83e\udd16 10.1.2 VLA Applications in Robotics \ud83e\udd16",id:"-1012-vla-applications-in-robotics-",level:3},{value:"\ud83e\udd16 10.2 Vision Processing for Robot Control \ud83e\udd16",id:"-102-vision-processing-for-robot-control-",level:2},{value:"\u2139\ufe0f 10.2.1 Object Detection and Recognition \u2139\ufe0f",id:"\u2139\ufe0f-1021-object-detection-and-recognition-\u2139\ufe0f",level:3},{value:"\u2139\ufe0f 10.2.2 Scene Understanding and Spatial Reasoning \u2139\ufe0f",id:"\u2139\ufe0f-1022-scene-understanding-and-spatial-reasoning-\u2139\ufe0f",level:3},{value:"\u2139\ufe0f 10.2.3 Visual Goal-Conditioned Policies \u2139\ufe0f",id:"\u2139\ufe0f-1023-visual-goal-conditioned-policies-\u2139\ufe0f",level:3},{value:"\ud83e\udd16 10.3 Language Processing for Robot Control \ud83e\udd16",id:"-103-language-processing-for-robot-control-",level:2},{value:"\ud83d\udcac 10.3.1 Natural Language Understanding for Commands \ud83d\udcac",id:"-1031-natural-language-understanding-for-commands-",level:3},{value:"\ud83d\udcac 10.3.2 Large Language Model Integration \ud83d\udcac",id:"-1032-large-language-model-integration-",level:3},{value:"\ud83d\udcac 10.3.3 Multimodal Language Models \ud83d\udcac",id:"-1033-multimodal-language-models-",level:3},{value:"\u26a1 10.4 Action Execution and Control \u26a1",id:"-104-action-execution-and-control-",level:2},{value:"\ud83d\udcac 10.4.1 Mapping Language Concepts to Actions \ud83d\udcac",id:"-1041-mapping-language-concepts-to-actions-",level:3},{value:"\u2139\ufe0f 10.4.2 Hierarchical Task Planning \u2139\ufe0f",id:"\u2139\ufe0f-1042-hierarchical-task-planning-\u2139\ufe0f",level:3},{value:"\u2139\ufe0f 10.4.3 Safety and Validation \u2139\ufe0f",id:"\u2139\ufe0f-1043-safety-and-validation-\u2139\ufe0f",level:3},{value:"\ud83d\udd28 10.5 Real-World Implementation Examples \ud83d\udd28",id:"-105-real-world-implementation-examples-",level:2},{value:"\ud83e\udd16 10.5.1 Voice Command to Robot Action Pipeline \ud83e\udd16",id:"-1051-voice-command-to-robot-action-pipeline-",level:3},{value:"\ud83d\udc41\ufe0f 10.5.2 Vision-Guided Manipulation with Language Understanding \ud83d\udc41\ufe0f",id:"\ufe0f-1052-vision-guided-manipulation-with-language-understanding-\ufe0f",level:3},{value:"\ud83d\udcca 10.6 Evaluation and Assessment \ud83d\udcca",id:"-106-evaluation-and-assessment-",level:2},{value:"\ud83d\udcc8 10.6.1 VLA System Performance Metrics \ud83d\udcc8",id:"-1061-vla-system-performance-metrics-",level:3},{value:"\ud83d\udcdd 10.7 Summary \ud83d\udcdd",id:"-107-summary-",level:2},{value:"\u2139\ufe0f Key Takeaways: \u2139\ufe0f",id:"\u2139\ufe0f-key-takeaways-\u2139\ufe0f",level:3},{value:"\ud83e\udd14 Knowledge Check \ud83e\udd14",id:"-knowledge-check-",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"-chapter-10-reinforcement-learning-for-control-",children:"\ud83d\udcda Chapter 10: Reinforcement Learning for Control \ud83d\udcda"}),"\n",(0,t.jsx)(e.h2,{id:"-learning-objectives-",children:"\ud83c\udfaf Learning Objectives \ud83c\udfaf"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate visual perception, language understanding, and physical action in robotic systems"}),"\n",(0,t.jsx)(e.li,{children:"Implement Vision-Language-Action (VLA) models for robot control"}),"\n",(0,t.jsx)(e.li,{children:"Design multimodal neural architectures that process vision and language inputs"}),"\n",(0,t.jsx)(e.li,{children:"Connect large language models (LLMs) to robot action spaces"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate VLA systems for accuracy, safety, and efficiency"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"-101-introduction-to-vision-language-action-integration-",children:"\ud83d\udc4b 10.1 Introduction to Vision-Language-Action Integration \ud83d\udc4b"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language-Action (VLA) integration represents the convergence of three critical technologies in embodied AI: computer vision for understanding the environment, natural language processing for understanding commands and context, and robotic action execution for physical interaction. This integration enables robots to understand and respond to complex human instructions in real-world settings."}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-1011-the-vla-framework-\u2139\ufe0f",children:"\u2139\ufe0f 10.1.1 The VLA Framework \u2139\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"The VLA framework combines:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Vision Processing"}),": Understanding the robot's environment through cameras, LiDAR, and other sensors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language Understanding"}),": Interpreting human commands and natural language requests"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Execution"}),": Converting high-level goals into specific robot motor commands"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\n\r\nclass VLAModel(nn.Module):\r\n    def __init__(self, vision_encoder, language_encoder, action_decoder, hidden_dim=512):\r\n        super(VLAModel, self).__init__()\r\n        \r\n        self.vision_encoder = vision_encoder\r\n        self.language_encoder = language_encoder\r\n        self.action_decoder = action_decoder\r\n        \r\n        # Fusion layer to combine vision and language features\r\n        self.fusion_layer = nn.Linear(\r\n            vision_encoder.feature_dim + language_encoder.feature_dim, \r\n            hidden_dim\r\n        )\r\n        \r\n        # Final output layer for action prediction\r\n        self.action_predictor = nn.Linear(hidden_dim, action_decoder.action_dim)\r\n        \r\n    def forward(self, image, text_tokens):\r\n        # Encode vision input\r\n        vision_features = self.vision_encoder(image)\r\n        \r\n        # Encode language input\r\n        language_features = self.language_encoder(text_tokens)\r\n        \r\n        # Concatenate vision and language features\r\n        combined_features = torch.cat([vision_features, language_features], dim=-1)\r\n        \r\n        # Fuse features\r\n        fused_features = F.relu(self.fusion_layer(combined_features))\r\n        \r\n        # Predict actions\r\n        actions = self.action_predictor(fused_features)\r\n        \r\n        return actions\n"})}),"\n",(0,t.jsx)(e.h3,{id:"-1012-vla-applications-in-robotics-",children:"\ud83e\udd16 10.1.2 VLA Applications in Robotics \ud83e\udd16"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Conversational Robotics"}),": Robots that can understand and respond to natural language commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Planning"}),": Converting high-level language instructions into sequences of robot actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human-Robot Interaction"}),": Enabling natural communication between humans and robots"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Instruction Following"}),": Executing complex multi-step tasks based on human instructions"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"-102-vision-processing-for-robot-control-",children:"\ud83e\udd16 10.2 Vision Processing for Robot Control \ud83e\udd16"}),"\n",(0,t.jsx)(e.p,{children:"Robust vision processing is essential for robots to understand their environment and execute vision-guided actions."}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-1021-object-detection-and-recognition-\u2139\ufe0f",children:"\u2139\ufe0f 10.2.1 Object Detection and Recognition \u2139\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"Object detection provides robots with the ability to recognize and locate objects in their environment:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torchvision\r\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\r\n\r\nclass ObjectDetectionModule(nn.Module):\r\n    def __init__(self, num_classes=91, confidence_threshold=0.5):\r\n        super(ObjectDetectionModule, self).__init__()\r\n        \r\n        # Load pre-trained Faster R-CNN model\r\n        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\r\n        \r\n        # Replace the classifier with a new one for our specific classes\r\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\r\n        self.model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\r\n            in_features, num_classes\r\n        )\r\n        \r\n        self.confidence_threshold = confidence_threshold\r\n\r\n    def forward(self, images):\r\n        if self.training:\r\n            return self.model(images)\r\n        else:\r\n            # Inference mode\r\n            self.model.eval()\r\n            with torch.no_grad():\r\n                predictions = self.model(images)\r\n            \r\n            # Filter predictions by confidence\r\n            filtered_predictions = []\r\n            for prediction in predictions:\r\n                keep_indices = prediction['scores'] >= self.confidence_threshold\r\n                filtered_prediction = {\r\n                    'boxes': prediction['boxes'][keep_indices],\r\n                    'labels': prediction['labels'][keep_indices],\r\n                    'scores': prediction['scores'][keep_indices]\r\n                }\r\n                filtered_predictions.append(filtered_prediction)\r\n            \r\n            return filtered_predictions\r\n\r\n    def get_objects_with_descriptions(self, image, class_names):\r\n        \"\"\"Get detected objects with their descriptions\"\"\"\r\n        predictions = self(image)\r\n        \r\n        objects = []\r\n        for i, box in enumerate(predictions[0]['boxes']):\r\n            label = predictions[0]['labels'][i].item()\r\n            score = predictions[0]['scores'][i].item()\r\n            \r\n            if label < len(class_names):\r\n                object_info = {\r\n                    'name': class_names[label],\r\n                    'bbox': box.tolist(),\r\n                    'confidence': score\r\n                }\r\n                objects.append(object_info)\r\n        \r\n        return objects\n"})}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-1022-scene-understanding-and-spatial-reasoning-\u2139\ufe0f",children:"\u2139\ufe0f 10.2.2 Scene Understanding and Spatial Reasoning \u2139\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"For robots to navigate and interact with their environment, they need to understand spatial relationships:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torchvision.models as models\r\n\r\nclass SpatialReasoningModule(nn.Module):\r\n    def __init__(self, feature_dim=512):\r\n        super(SpatialReasoningModule, self).__init__()\r\n        \r\n        # Feature extractor for scene understanding\r\n        self.feature_extractor = models.resnet18(pretrained=True)\r\n        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, feature_dim)\r\n        \r\n        # Spatial relationship predictor\r\n        self.spatial_predictor = nn.Sequential(\r\n            nn.Linear(feature_dim * 2, feature_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(feature_dim, 128),  # 128 possible spatial relationships (e.g., near, far, left, right, on top of, etc.)\r\n            nn.Softmax(dim=1)\r\n        )\r\n\r\n    def forward(self, image):\r\n        features = self.feature_extractor(image)\r\n        return features\r\n\r\n    def predict_spatial_relationship(self, obj1_features, obj2_features):\r\n        """Predict spatial relationship between two objects"""\r\n        combined_features = torch.cat([obj1_features, obj2_features], dim=1)\r\n        relationships = self.spatial_predictor(combined_features)\r\n        return relationships\r\n\r\n    def find_object_location(self, image, target_object):\r\n        """Find the location of a specific object in an image"""\r\n        # This would typically involve a combination of object detection and spatial reasoning\r\n        features = self(image)\r\n        # Implementation would depend on the specific spatial reasoning approach\r\n        return features  # Placeholder\n'})}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-1023-visual-goal-conditioned-policies-\u2139\ufe0f",children:"\u2139\ufe0f 10.2.3 Visual Goal-Conditioned Policies \u2139\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"Robots need to learn to reach visual goals specified by images or descriptions:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass VisualGoalConditionedPolicy(nn.Module):\r\n    def __init__(self, observation_dim, action_dim, goal_dim, hidden_dim=256):\r\n        super(VisualGoalConditionedPolicy, self).__init__()\r\n        \r\n        # Visual encoder for current state\r\n        self.state_encoder = nn.Sequential(\r\n            nn.Linear(observation_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Visual encoder for goal\r\n        self.goal_encoder = nn.Sequential(\r\n            nn.Linear(goal_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Actor network for action prediction\r\n        self.actor = nn.Sequential(\r\n            nn.Linear(hidden_dim * 2, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim),\r\n            nn.Tanh()  # Bound actions to [-1, 1]\r\n        )\r\n        \r\n        # Critic network for value prediction\r\n        self.critic = nn.Sequential(\r\n            nn.Linear(hidden_dim * 2, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, 1)\r\n        )\r\n\r\n    def forward(self, state, goal):\r\n        encoded_state = self.state_encoder(state)\r\n        encoded_goal = self.goal_encoder(goal)\r\n        \r\n        # Concatenate state and goal representations\r\n        combined = torch.cat([encoded_state, encoded_goal], dim=1)\r\n        \r\n        # Predict action and value\r\n        action = self.actor(combined)\r\n        value = self.critic(combined)\r\n        \r\n        return action, value\r\n\r\n    def get_action(self, state, goal):\r\n        """Get action for given state and goal"""\r\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\r\n        goal_tensor = torch.FloatTensor(goal).unsqueeze(0)\r\n        \r\n        action, _ = self.forward(state_tensor, goal_tensor)\r\n        \r\n        return action.detach().cpu().numpy()[0]\n'})}),"\n",(0,t.jsx)(e.h2,{id:"-103-language-processing-for-robot-control-",children:"\ud83e\udd16 10.3 Language Processing for Robot Control \ud83e\udd16"}),"\n",(0,t.jsx)(e.h3,{id:"-1031-natural-language-understanding-for-commands-",children:"\ud83d\udcac 10.3.1 Natural Language Understanding for Commands \ud83d\udcac"}),"\n",(0,t.jsx)(e.p,{children:"Robots need to interpret natural language commands and convert them to executable actions:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport numpy as np\r\nfrom transformers import AutoTokenizer, AutoModel\r\n\r\nclass CommandUnderstandingModel(nn.Module):\r\n    def __init__(self, vocab_size, hidden_dim=256, action_dim=20):\r\n        super(CommandUnderstandingModel, self).__init__()\r\n        \r\n        # Use a pre-trained transformer model (e.g., BERT, RoBERTa) for language understanding\r\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\r\n        self.language_model = AutoModel.from_pretrained('bert-base-uncased')\r\n        \r\n        # Freeze the transformer parameters initially\r\n        for param in self.language_model.parameters():\r\n            param.requires_grad = False\r\n        \r\n        # Custom layers for command understanding\r\n        self.command_encoder = nn.Sequential(\r\n            nn.Linear(self.language_model.config.hidden_size, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Output heads for different aspects of command understanding\r\n        self.action_predictor = nn.Linear(hidden_dim, action_dim)\r\n        self.object_detector = nn.Linear(hidden_dim, 100)  # 100 possible objects\r\n        self.spatial_processor = nn.Linear(hidden_dim, 50)  # 50 spatial relations\r\n\r\n    def forward(self, input_ids, attention_mask):\r\n        # Get language embeddings\r\n        outputs = self.language_model(input_ids=input_ids, attention_mask=attention_mask)\r\n        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]\r\n        \r\n        # Process the language embedding\r\n        encoded_command = self.command_encoder(pooled_output)\r\n        \r\n        # Generate different outputs\r\n        actions = self.action_predictor(encoded_command)\r\n        objects = self.object_detector(encoded_command)\r\n        spatial_relations = self.spatial_processor(encoded_command)\r\n        \r\n        return actions, objects, spatial_relations\r\n\r\n    def process_text_command(self, text_command):\r\n        \"\"\"Process a natural language command\"\"\"\r\n        # Tokenize the input text\r\n        inputs = self.tokenizer(\r\n            text_command, \r\n            return_tensors=\"pt\", \r\n            padding=True, \r\n            truncation=True, \r\n            max_length=128\r\n        )\r\n        \r\n        # Forward pass\r\n        actions, objects, spatial_relations = self.forward(\r\n            inputs['input_ids'], \r\n            inputs['attention_mask']\r\n        )\r\n        \r\n        return {\r\n            'actions': actions,\r\n            'objects': objects,\r\n            'spatial': spatial_relations\r\n        }\r\n\r\n# \ud83e\udd16 Example robot command vocabulary \ud83e\udd16\r\nclass RobotCommandProcessor:\r\n    def __init__(self):\r\n        self.action_keywords = {\r\n            'move': ['go', 'move', 'walk', 'navigate', 'approach'],\r\n            'grasp': ['grasp', 'pick', 'take', 'grab', 'hold'],\r\n            'place': ['place', 'put', 'set', 'drop', 'release'],\r\n            'inspect': ['look', 'see', 'examine', 'check', 'inspect'],\r\n            'follow': ['follow', 'track', 'accompany']\r\n        }\r\n        \r\n        self.object_identifiers = {\r\n            'object': ['object', 'item', 'thing', 'it'],\r\n            'person': ['person', 'human', 'you', 'me', 'someone'],\r\n            'location': ['location', 'place', 'spot', 'area', 'there', 'here']\r\n        }\r\n\r\n    def parse_command(self, command):\r\n        \"\"\"Parse a natural language command into structured representation\"\"\"\r\n        command_lower = command.lower()\r\n        \r\n        # Identify action\r\n        action = None\r\n        for action_type, keywords in self.action_keywords.items():\r\n            if any(keyword in command_lower for keyword in keywords):\r\n                action = action_type\r\n                break\r\n        \r\n        # Identify objects and locations\r\n        objects = []\r\n        for obj_type, identifiers in self.object_identifiers.items():\r\n            if any(identifier in command_lower for identifier in identifiers):\r\n                objects.append(obj_type)\r\n        \r\n        # Extract spatial relationships\r\n        spatial_keywords = ['to', 'toward', 'near', 'by', 'next', 'left', 'right', 'front', 'back']\r\n        spatial_relations = [word for word in command_lower.split() if word in spatial_keywords]\r\n        \r\n        return {\r\n            'action': action,\r\n            'objects': objects,\r\n            'spatial_relations': spatial_relations,\r\n            'original_command': command\r\n        }\n"})}),"\n",(0,t.jsx)(e.h3,{id:"-1032-large-language-model-integration-",children:"\ud83d\udcac 10.3.2 Large Language Model Integration \ud83d\udcac"}),"\n",(0,t.jsx)(e.p,{children:"Connecting large language models to robot action spaces enables complex task planning:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport openai\r\nfrom typing import List, Dict, Any\r\n\r\nclass LLMRobotController:\r\n    def __init__(self, llm_model_name=\"gpt-3.5-turbo\", robot_action_space=None):\r\n        self.llm_model_name = llm_model_name\r\n        self.robot_action_space = robot_action_space or self._default_action_space()\r\n        \r\n        # Define robot capabilities and their API endpoints\r\n        self.robot_capabilities = {\r\n            'navigation': {\r\n                'actions': ['move_to', 'go_to', 'navigate_to'],\r\n                'params': ['x', 'y', 'location_name']\r\n            },\r\n            'manipulation': {\r\n                'actions': ['pick_up', 'place', 'grasp', 'release'],\r\n                'params': ['object_name', 'x', 'y', 'z']\r\n            },\r\n            'perception': {\r\n                'actions': ['look_at', 'find', 'identify', 'scan'],\r\n                'params': ['object_name', 'location']\r\n            },\r\n            'communication': {\r\n                'actions': ['speak', 'listen', 'communicate'],\r\n                'params': ['text']\r\n            }\r\n        }\r\n\r\n    def _default_action_space(self):\r\n        \"\"\"Define default robot action space\"\"\"\r\n        return {\r\n            'navigation': {\r\n                'move_to': {'params': ['target_location']},\r\n                'go_to': {'params': ['x', 'y', 'z', 'orientation']},\r\n                'navigate_to': {'params': ['location_name']}\r\n            },\r\n            'manipulation': {\r\n                'pick_up': {'params': ['object_id', 'location']},\r\n                'place': {'params': ['object_id', 'target_location']},\r\n                'grasp': {'params': ['object_id']},\r\n                'release': {'params': ['object_id']}\r\n            },\r\n            'perception': {\r\n                'look_at': {'params': ['target_location']},\r\n                'find': {'params': ['object_name']},\r\n                'identify': {'params': ['target_location']},\r\n                'scan': {'params': ['area']}\r\n            },\r\n            'communication': {\r\n                'speak': {'params': ['text']},\r\n                'listen': {'params': []},\r\n                'communicate': {'params': ['target', 'message']}\r\n            }\r\n        }\r\n\r\n    def plan_from_natural_language(self, user_command: str) -> List[Dict[str, Any]]:\r\n        \"\"\"Convert natural language command to robot action plan\"\"\"\r\n        # Create a structured prompt for the LLM\r\n        prompt = f\"\"\"\r\n        Convert the following human command into a sequence of robot actions.\r\n        Robot capabilities: {list(self.robot_capabilities.keys())}\r\n        \r\n        Human command: \"{user_command}\"\r\n        \r\n        Return the plan as a JSON list of actions with the following format:\r\n        [\r\n            {{\r\n                \"action\": \"action_name\",\r\n                \"params\": {{\"param_name\": \"param_value\", ...}}\r\n            }}\r\n        ]\r\n        \r\n        Only use actions from the robot's capabilities. Be specific with parameters.\r\n        \"\"\"\r\n        \r\n        try:\r\n            # Call the LLM API (in practice, you'd use the actual API)\r\n            response = self._mock_llm_call(prompt)  # Placeholder for actual LLM call\r\n            \r\n            # Parse the response\r\n            action_plan = self._parse_action_plan(response)\r\n            \r\n            # Validate and adapt plan to robot capabilities\r\n            validated_plan = self._validate_plan(action_plan)\r\n            \r\n            return validated_plan\r\n            \r\n        except Exception as e:\r\n            print(f\"Error planning from natural language: {e}\")\r\n            return []\r\n\r\n    def _mock_llm_call(self, prompt: str) -> str:\r\n        \"\"\"Mock implementation of LLM call\"\"\"\r\n        # In practice, this would call an actual LLM API\r\n        # For example: openai.ChatCompletion.create(...)\r\n        \r\n        # Simulate a response based on common commands\r\n        if \"bring me\" in prompt.lower() or \"pick up\" in prompt.lower():\r\n            return '''\r\n            [\r\n                {\"action\": \"find\", \"params\": {\"object_name\": \"water bottle\"}},\r\n                {\"action\": \"navigate_to\", \"params\": {\"location_name\": \"kitchen counter\"}},\r\n                {\"action\": \"pick_up\", \"params\": {\"object_id\": \"water bottle\", \"location\": \"kitchen counter\"}},\r\n                {\"action\": \"navigate_to\", \"params\": {\"location_name\": \"user location\"}},\r\n                {\"action\": \"place\", \"params\": {\"object_id\": \"water bottle\", \"target_location\": \"user location\"}}\r\n            ]\r\n            '''\r\n        elif \"go to\" in prompt.lower() or \"navigate to\" in prompt.lower():\r\n            return '''\r\n            [\r\n                {\"action\": \"navigate_to\", \"params\": {\"location_name\": \"living room\"}}\r\n            ]\r\n            '''\r\n        else:\r\n            return '''\r\n            [\r\n                {\"action\": \"listen\", \"params\": {}},\r\n                {\"action\": \"speak\", \"params\": {\"text\": \"I didn't understand that command. Can you please rephrase?\"}}\r\n            ]\r\n            '''\r\n\r\n    def _parse_action_plan(self, response: str) -> List[Dict[str, Any]]:\r\n        \"\"\"Parse the LLM response into an action plan\"\"\"\r\n        import json\r\n        \r\n        try:\r\n            # In practice, you'd parse the actual JSON response\r\n            plan = json.loads(response)\r\n            return plan\r\n        except json.JSONDecodeError:\r\n            print(\"LLM response is not valid JSON\")\r\n            return []\r\n\r\n    def _validate_plan(self, plan: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\r\n        \"\"\"Validate and adapt the action plan to robot capabilities\"\"\"\r\n        validated_plan = []\r\n        \r\n        for action_step in plan:\r\n            action_name = action_step.get(\"action\")\r\n            params = action_step.get(\"params\", {})\r\n            \r\n            # Check if action exists in robot capabilities\r\n            action_valid = False\r\n            for category, actions in self.robot_action_space.items():\r\n                if action_name in actions:\r\n                    # Validate parameters\r\n                    required_params = actions[action_name]['params']\r\n                    missing_params = [p for p in required_params if p not in params]\r\n                    \r\n                    if missing_params:\r\n                        print(f\"Warning: Missing parameters {missing_params} for action {action_name}\")\r\n                    \r\n                    validated_plan.append({\r\n                        \"action\": action_name,\r\n                        \"params\": params,\r\n                        \"validated\": True\r\n                    })\r\n                    action_valid = True\r\n                    break\r\n            \r\n            if not action_valid:\r\n                print(f\"Warning: Unknown action {action_name}, skipping\")\r\n        \r\n        return validated_plan\r\n\r\n    def execute_plan(self, plan: List[Dict[str, Any]]) -> bool:\r\n        \"\"\"Execute the validated action plan\"\"\"\r\n        for i, action_step in enumerate(plan):\r\n            action_name = action_step[\"action\"]\r\n            params = action_step[\"params\"]\r\n            \r\n            print(f\"Executing action {i+1}/{len(plan)}: {action_name}\")\r\n            \r\n            # In a real implementation, this would call the robot's actual action API\r\n            success = self._execute_robot_action(action_name, params)\r\n            \r\n            if not success:\r\n                print(f\"Action {action_name} failed, stopping execution\")\r\n                return False\r\n        \r\n        print(\"Plan execution completed successfully\")\r\n        return True\r\n\r\n    def _execute_robot_action(self, action_name: str, params: Dict[str, Any]) -> bool:\r\n        \"\"\"Execute a single robot action (placeholder implementation)\"\"\"\r\n        # This would interface with the actual robot control system\r\n        print(f\"  -> {action_name} with params: {params}\")\r\n        \r\n        # Simulate action execution\r\n        import time\r\n        time.sleep(0.5)  # Simulate action time\r\n        \r\n        # Return success status\r\n        return True  # Simulate success\n"})}),"\n",(0,t.jsx)(e.h3,{id:"-1033-multimodal-language-models-",children:"\ud83d\udcac 10.3.3 Multimodal Language Models \ud83d\udcac"}),"\n",(0,t.jsx)(e.p,{children:"Advanced VLA systems use multimodal models that process both visual and text inputs simultaneously:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\n\r\nclass MultimodalTransformer(nn.Module):\r\n    def __init__(self, vision_dim=512, text_dim=768, hidden_dim=1024, num_heads=8, num_layers=6):\r\n        super(MultimodalTransformer, self).__init__()\r\n        \r\n        self.vision_dim = vision_dim\r\n        self.text_dim = text_dim\r\n        self.hidden_dim = hidden_dim\r\n        \r\n        # Projection layers to map both modalities to the same space\r\n        self.vision_projection = nn.Linear(vision_dim, hidden_dim)\r\n        self.text_projection = nn.Linear(text_dim, hidden_dim)\r\n        \r\n        # Positional encodings\r\n        self.vision_pos_encoding = nn.Parameter(torch.randn(1, 50, hidden_dim))  # 50 vision tokens\r\n        self.text_pos_encoding = nn.Parameter(torch.randn(1, 32, hidden_dim))    # 32 text tokens\r\n        \r\n        # Transformer layers for multimodal fusion\r\n        encoder_layer = nn.TransformerEncoderLayer(\r\n            d_model=hidden_dim,\r\n            nhead=num_heads,\r\n            dim_feedforward=hidden_dim * 4,\r\n            batch_first=True\r\n        )\r\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\r\n        \r\n        # Output layers for different tasks\r\n        self.action_head = nn.Linear(hidden_dim, 50)  # 50 possible actions\r\n        self.object_head = nn.Linear(hidden_dim, 100) # 100 possible objects\r\n        self.relation_head = nn.Linear(hidden_dim, 50) # 50 spatial/semantic relations\r\n\r\n    def forward(self, vision_features, text_features):\r\n        # Project vision and text to the same space\r\n        vision_projected = self.vision_projection(vision_features)\r\n        text_projected = self.text_projection(text_features)\r\n        \r\n        # Add positional encodings\r\n        # Note: In practice, you\'d need to ensure the dimensions match the positional encodings\r\n        batch_size = vision_projected.size(0)\r\n        if vision_projected.size(1) < self.vision_pos_encoding.size(1):\r\n            # Pad if necessary\r\n            pad_len = self.vision_pos_encoding.size(1) - vision_projected.size(1)\r\n            vision_projected = F.pad(vision_projected, (0, 0, 0, pad_len), "constant", 0)\r\n        elif vision_projected.size(1) > self.vision_pos_encoding.size(1):\r\n            # Truncate if necessary\r\n            vision_projected = vision_projected[:, :self.vision_pos_encoding.size(1), :]\r\n            \r\n        if text_projected.size(1) < self.text_pos_encoding.size(1):\r\n            pad_len = self.text_pos_encoding.size(1) - text_projected.size(1)\r\n            text_projected = F.pad(text_projected, (0, 0, 0, pad_len), "constant", 0)\r\n        elif text_projected.size(1) > self.text_pos_encoding.size(1):\r\n            text_projected = text_projected[:, :self.text_pos_encoding.size(1), :]\r\n        \r\n        vision_with_pos = vision_projected + self.vision_pos_encoding[:, :vision_projected.size(1), :]\r\n        text_with_pos = text_projected + self.text_pos_encoding[:, :text_projected.size(1), :]\r\n        \r\n        # Concatenate vision and text features\r\n        combined_features = torch.cat([vision_with_pos, text_with_pos], dim=1)\r\n        \r\n        # Process with transformer\r\n        multimodal_features = self.transformer(combined_features)\r\n        \r\n        # Use the [CLS] token representation (first token) for classification tasks\r\n        cls_features = multimodal_features[:, 0, :]\r\n        \r\n        # Generate outputs for different tasks\r\n        actions = self.action_head(cls_features)\r\n        objects = self.object_head(cls_features)\r\n        relations = self.relation_head(cls_features)\r\n        \r\n        return {\r\n            \'actions\': actions,\r\n            \'objects\': objects, \r\n            \'relations\': relations\r\n        }\r\n\r\nclass VLAIntegrationModule:\r\n    def __init__(self, multimodal_model):\r\n        self.model = multimodal_model\r\n        \r\n        # Vision encoder (e.g., ResNet, ViT)\r\n        self.vision_encoder = self._build_vision_encoder()\r\n        \r\n        # Text encoder (e.g., BERT, RoBERTa)\r\n        self.text_encoder = self._build_text_encoder()\r\n        \r\n    def _build_vision_encoder(self):\r\n        """Build vision encoder (placeholder)"""\r\n        import torchvision.models as models\r\n        model = models.resnet18(pretrained=True)\r\n        # Remove the final classification layer\r\n        features_dim = model.fc.in_features\r\n        model.fc = nn.Identity()\r\n        return model\r\n    \r\n    def _build_text_encoder(self):\r\n        """Build text encoder (placeholder)"""\r\n        from transformers import AutoTokenizer, AutoModel\r\n        model_name = "bert-base-uncased"\r\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n        model = AutoModel.from_pretrained(model_name)\r\n        return {\'model\': model, \'tokenizer\': tokenizer}\r\n    \r\n    def process_vision_language_input(self, image, text):\r\n        """Process combined vision and language input"""\r\n        # Encode vision\r\n        with torch.no_grad():\r\n            vision_features = self.vision_encoder(image)\r\n        \r\n        # Encode text\r\n        text_tokens = self.text_encoder[\'tokenizer\'](\r\n            text, \r\n            return_tensors="pt", \r\n            padding=True, \r\n            truncation=True, \r\n            max_length=128\r\n        )\r\n        \r\n        with torch.no_grad():\r\n            text_outputs = self.text_encoder[\'model\'](\r\n                input_ids=text_tokens[\'input_ids\'],\r\n                attention_mask=text_tokens[\'attention_mask\']\r\n            )\r\n            # Use the pooled output (CLS token)\r\n            text_features = text_outputs.pooler_output\r\n        \r\n        # Pass to multimodal model\r\n        outputs = self.model(vision_features.unsqueeze(1), text_features.unsqueeze(1))\r\n        \r\n        return outputs\n'})}),"\n",(0,t.jsx)(e.h2,{id:"-104-action-execution-and-control-",children:"\u26a1 10.4 Action Execution and Control \u26a1"}),"\n",(0,t.jsx)(e.h3,{id:"-1041-mapping-language-concepts-to-actions-",children:"\ud83d\udcac 10.4.1 Mapping Language Concepts to Actions \ud83d\udcac"}),"\n",(0,t.jsx)(e.p,{children:"Creating mappings between high-level language commands and low-level robot actions:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import numpy as np\r\nfrom enum import Enum\r\n\r\nclass RobotAction(Enum):\r\n    MOVE_TO = \"move_to\"\r\n    GRASP = \"grasp\"\r\n    PLACE = \"place\"\r\n    INSPET = \"inspect\"\r\n    SPEAK = \"speak\"\r\n    LISTEN = \"listen\"\r\n\r\nclass LanguageToActionMapper:\r\n    def __init__(self, robot_interface):\r\n        self.robot = robot_interface\r\n        self.action_templates = self._create_action_templates()\r\n        self.action_space = self._define_action_space()\r\n        \r\n    def _create_action_templates(self):\r\n        \"\"\"Define templates for converting language to actions\"\"\"\r\n        return {\r\n            'move': {\r\n                'variants': ['go to', 'move to', 'navigate to', 'approach'],\r\n                'action': RobotAction.MOVE_TO,\r\n                'slots': ['target_location']\r\n            },\r\n            'manipulation': {\r\n                'variants': ['pick up', 'grasp', 'take', 'pick', 'hold'],\r\n                'action': RobotAction.GRASP,\r\n                'slots': ['object_id', 'object_type']\r\n            },\r\n            'place': {\r\n                'variants': ['place', 'put', 'set down', 'release'],\r\n                'action': RobotAction.PLACE,\r\n                'slots': ['object_id', 'target_location']\r\n            },\r\n            'communication': {\r\n                'variants': ['say', 'speak', 'tell', 'communicate'],\r\n                'action': RobotAction.SPEAK,\r\n                'slots': ['text']\r\n            }\r\n        }\r\n    \r\n    def _define_action_space(self):\r\n        \"\"\"Define the robot's action space\"\"\"\r\n        return {\r\n            RobotAction.MOVE_TO: {\r\n                'params': ['x', 'y', 'z', 'orientation'],\r\n                'constraints': {\r\n                    'x': (-10.0, 10.0),\r\n                    'y': (-10.0, 10.0),\r\n                    'z': (0.0, 2.0),\r\n                    'orientation': (0.0, 360.0)\r\n                }\r\n            },\r\n            RobotAction.GRASP: {\r\n                'params': ['object_id', 'grasp_type', 'force'],\r\n                'constraints': {\r\n                    'grasp_type': ['precision', 'power'],\r\n                    'force': (0.0, 100.0)\r\n                }\r\n            },\r\n            RobotAction.PLACE: {\r\n                'params': ['x', 'y', 'z', 'object_id'],\r\n                'constraints': {\r\n                    'x': (-10.0, 10.0),\r\n                    'y': (-10.0, 10.0),\r\n                    'z': (0.0, 2.0)\r\n                }\r\n            },\r\n            RobotAction.SPEAK: {\r\n                'params': ['text'],\r\n                'constraints': {\r\n                    'text': str\r\n                }\r\n            }\r\n        }\r\n    \r\n    def parse_command_to_action(self, command):\r\n        \"\"\"Convert natural language command to robot action\"\"\"\r\n        command_lower = command.lower()\r\n        \r\n        # Find matching action template\r\n        for action_type, template in self.action_templates.items():\r\n            for variant in template['variants']:\r\n                if variant in command_lower:\r\n                    action = template['action']\r\n                    params = self._extract_parameters(command_lower, template)\r\n                    return self._validate_action(action, params)\r\n        \r\n        # If no specific action found, default to communication\r\n        return {\r\n            'action': RobotAction.SPEAK,\r\n            'params': {'text': f\"I don't understand the command: {command}\"}\r\n        }\r\n    \r\n    def _extract_parameters(self, command, template):\r\n        \"\"\"Extract action parameters from command\"\"\"\r\n        params = {}\r\n        \r\n        # Extract location information\r\n        location_keywords = ['kitchen', 'living room', 'bedroom', 'table', 'counter', 'shelf']\r\n        for keyword in location_keywords:\r\n            if keyword in command:\r\n                params['target_location'] = keyword\r\n                break\r\n        \r\n        # Extract object information\r\n        object_keywords = ['bottle', 'cup', 'book', 'phone', 'box', 'object']\r\n        for keyword in object_keywords:\r\n            if keyword in command:\r\n                params['object_id'] = keyword\r\n                params['object_type'] = keyword\r\n                break\r\n        \r\n        # Extract text for communication\r\n        if template['action'] == RobotAction.SPEAK:\r\n            # Remove command words to get the message\r\n            import re\r\n            message = re.sub(r'(say|speak|tell|communicate)\\s+', '', command, flags=re.IGNORECASE)\r\n            params['text'] = message.strip()\r\n        \r\n        return params\r\n    \r\n    def _validate_action(self, action, params):\r\n        \"\"\"Validate action parameters against constraints\"\"\"\r\n        if action not in self.action_space:\r\n            return None\r\n        \r\n        action_def = self.action_space[action]\r\n        validated_params = {}\r\n        \r\n        for param in action_def['params']:\r\n            if param in params:\r\n                value = params[param]\r\n                constraint = action_def.get('constraints', {}).get(param)\r\n                \r\n                if constraint:\r\n                    if isinstance(constraint, tuple):  # Range constraint\r\n                        if isinstance(value, (int, float)):\r\n                            validated_params[param] = max(min(value, constraint[1]), constraint[0])\r\n                        else:\r\n                            validated_params[param] = value  # Keep as is for non-numeric values\r\n                    elif isinstance(constraint, list):  # Categorical constraint\r\n                        if value in constraint:\r\n                            validated_params[param] = value\r\n                        else:\r\n                            validated_params[param] = constraint[0]  # Default to first option\r\n                    elif constraint == str:  # Type constraint\r\n                        validated_params[param] = str(value)\r\n                    else:\r\n                        validated_params[param] = value\r\n                else:\r\n                    validated_params[param] = value\r\n        \r\n        return {\r\n            'action': action,\r\n            'params': validated_params\r\n        }\r\n    \r\n    def execute_action(self, action_desc):\r\n        \"\"\"Execute a parsed action on the robot\"\"\"\r\n        action = action_desc['action']\r\n        params = action_desc['params']\r\n        \r\n        if action == RobotAction.MOVE_TO:\r\n            return self.robot.move_to(\r\n                x=params.get('x', 0.0),\r\n                y=params.get('y', 0.0),\r\n                z=params.get('z', 0.0),\r\n                orientation=params.get('orientation', 0.0)\r\n            )\r\n        elif action == RobotAction.GRASP:\r\n            return self.robot.grasp(\r\n                object_id=params.get('object_id', ''),\r\n                grasp_type=params.get('grasp_type', 'power'),\r\n                force=params.get('force', 50.0)\r\n            )\r\n        elif action == RobotAction.PLACE:\r\n            return self.robot.place(\r\n                x=params.get('x', 0.0),\r\n                y=params.get('y', 0.0),\r\n                z=params.get('z', 0.0),\r\n                object_id=params.get('object_id', '')\r\n            )\r\n        elif action == RobotAction.SPEAK:\r\n            return self.robot.speak(params.get('text', ''))\r\n        elif action == RobotAction.LISTEN:\r\n            return self.robot.listen()\r\n        else:\r\n            print(f\"Unknown action: {action}\")\r\n            return False\r\n\r\n# \ud83e\udd16 Example robot interface \ud83e\udd16\r\nclass RobotInterface:\r\n    def move_to(self, x, y, z, orientation):\r\n        \"\"\"Move robot to specified coordinates\"\"\"\r\n        print(f\"Moving to position ({x}, {y}, {z}) with orientation {orientation} degrees\")\r\n        # Implementation would interface with navigation stack\r\n        return True\r\n    \r\n    def grasp(self, object_id, grasp_type, force):\r\n        \"\"\"Grasp an object\"\"\"\r\n        print(f\"Grasping {object_id} with {grasp_type} grasp at {force}% force\")\r\n        # Implementation would interface with manipulation stack\r\n        return True\r\n    \r\n    def place(self, x, y, z, object_id):\r\n        \"\"\"Place an object at specified coordinates\"\"\"\r\n        print(f\"Placing {object_id} at position ({x}, {y}, {z})\")\r\n        # Implementation would interface with manipulation stack\r\n        return True\r\n    \r\n    def speak(self, text):\r\n        \"\"\"Make robot speak text\"\"\"\r\n        print(f\"Robot says: {text}\")\r\n        # Implementation would interface with TTS system\r\n        return True\r\n    \r\n    def listen(self):\r\n        \"\"\"Make robot listen for user input\"\"\"\r\n        print(\"Robot is listening...\")\r\n        # Implementation would interface with speech recognition\r\n        return \"user said something\"\n"})}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-1042-hierarchical-task-planning-\u2139\ufe0f",children:"\u2139\ufe0f 10.4.2 Hierarchical Task Planning \u2139\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"Complex tasks require hierarchical decomposition from high-level goals to low-level actions:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class HierarchicalTaskPlanner:\r\n    def __init__(self, low_level_controller):\r\n        self.low_level_controller = low_level_controller\r\n        self.task_library = self._build_task_library()\r\n    \r\n    def _build_task_library(self):\r\n        \"\"\"Build a library of high-level tasks and their decompositions\"\"\"\r\n        return {\r\n            'fetch_object': {\r\n                'description': 'Fetch an object from one location and bring it to another',\r\n                'subtasks': [\r\n                    {'action': 'find_object', 'params': ['object_type']},\r\n                    {'action': 'navigate_to', 'params': ['object_location']},\r\n                    {'action': 'grasp', 'params': ['object_id']},\r\n                    {'action': 'navigate_to', 'params': ['delivery_location']},\r\n                    {'action': 'place', 'params': ['object_id', 'delivery_location']}\r\n                ]\r\n            },\r\n            'clean_surface': {\r\n                'description': 'Clean a surface by picking up objects and disposing of them',\r\n                'subtasks': [\r\n                    {'action': 'scan_area', 'params': ['area']},\r\n                    {'action': 'identify_objects', 'params': ['area']},\r\n                    {'action': 'pick_up', 'params': ['object_id']},\r\n                    {'action': 'navigate_to', 'params': ['disposal_location']},\r\n                    {'action': 'place', 'params': ['object_id', 'disposal_location']},\r\n                    {'action': 'return_to', 'params': ['area']}\r\n                ]\r\n            },\r\n            'set_table': {\r\n                'description': 'Set a table with specific objects in specific locations',\r\n                'subtasks': [\r\n                    {'action': 'navigate_to', 'params': ['storage_area']},\r\n                    {'action': 'identify_objects', 'params': ['storage_area']},\r\n                    {'action': 'pick_up', 'params': ['object_id']},\r\n                    {'action': 'navigate_to', 'params': ['table']},\r\n                    {'action': 'place', 'params': ['object_id', 'table_position']},\r\n                    {'action': 'adjust_position', 'params': ['object_id', 'desired_position']}\r\n                ]\r\n            }\r\n        }\r\n    \r\n    def decompose_task(self, high_level_task, task_params):\r\n        \"\"\"Decompose a high-level task into a sequence of primitive actions\"\"\"\r\n        if high_level_task not in self.task_library:\r\n            return []\r\n        \r\n        subtasks = self.task_library[high_level_task]['subtasks']\r\n        action_sequence = []\r\n        \r\n        for subtask in subtasks:\r\n            action = subtask['action']\r\n            required_params = subtask['params']\r\n            \r\n            # Create action with available parameters\r\n            action_params = {}\r\n            for param in required_params:\r\n                if param in task_params:\r\n                    action_params[param] = task_params[param]\r\n            \r\n            action_sequence.append({\r\n                'action': action,\r\n                'params': action_params\r\n            })\r\n        \r\n        return action_sequence\r\n    \r\n    def execute_task(self, high_level_task, task_params):\r\n        \"\"\"Execute a high-level task by decomposing and executing subtasks\"\"\"\r\n        action_sequence = self.decompose_task(high_level_task, task_params)\r\n        \r\n        print(f\"Executing task: {high_level_task} with params: {task_params}\")\r\n        \r\n        for i, action in enumerate(action_sequence):\r\n            print(f\"Step {i+1}/{len(action_sequence)}: {action['action']}\")\r\n            \r\n            success = self.low_level_controller.execute_action(action)\r\n            \r\n            if not success:\r\n                print(f\"Task execution failed at step {i+1}\")\r\n                return False\r\n        \r\n        print(f\"Task {high_level_task} completed successfully\")\r\n        return True\r\n\r\nclass TaskExecutionMonitor:\r\n    def __init__(self):\r\n        self.current_task = None\r\n        self.current_step = 0\r\n        self.task_history = []\r\n        \r\n    def start_task(self, task_name, task_params):\r\n        \"\"\"Start monitoring a new task\"\"\"\r\n        self.current_task = {\r\n            'name': task_name,\r\n            'params': task_params,\r\n            'start_time': time.time(),\r\n            'steps': [],\r\n            'status': 'running'\r\n        }\r\n        self.current_step = 0\r\n        \r\n    def record_step(self, step_description, success=True):\r\n        \"\"\"Record the execution of a task step\"\"\"\r\n        if self.current_task:\r\n            step_record = {\r\n                'step': self.current_step,\r\n                'description': step_description,\r\n                'success': success,\r\n                'timestamp': time.time()\r\n            }\r\n            self.current_task['steps'].append(step_record)\r\n            self.current_step += 1\r\n    \r\n    def complete_task(self, success=True):\r\n        \"\"\"Complete the current task\"\"\"\r\n        if self.current_task:\r\n            self.current_task['end_time'] = time.time()\r\n            self.current_task['status'] = 'success' if success else 'failed'\r\n            self.current_task['duration'] = self.current_task['end_time'] - self.current_task['start_time']\r\n            \r\n            self.task_history.append(self.current_task)\r\n            self.current_task = None\r\n    \r\n    def get_task_report(self):\r\n        \"\"\"Generate a report of task execution\"\"\"\r\n        if not self.current_task and not self.task_history:\r\n            return \"No tasks executed yet.\"\r\n        \r\n        report = \"Task Execution Report:\\n\"\r\n        if self.current_task:\r\n            report += f\"Current task: {self.current_task['name']} - Status: {self.current_task['status']}\\n\"\r\n        \r\n        for task in self.task_history[-5:]:  # Last 5 tasks\r\n            report += f\"Task: {task['name']}, Status: {task['status']}, Duration: {task['duration']:.2f}s\\n\"\r\n            for step in task['steps']:\r\n                status = \"\u2713\" if step['success'] else \"\u2717\"\r\n                report += f\"  {status} Step {step['step']}: {step['description']}\\n\"\r\n        \r\n        return report\n"})}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-1043-safety-and-validation-\u2139\ufe0f",children:"\u2139\ufe0f 10.4.3 Safety and Validation \u2139\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"Safety is paramount when executing language-guided robot actions:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import threading\r\nimport time\r\n\r\nclass SafeVLAController:\r\n    def __init__(self, robot_controller, safety_thresholds=None):\r\n        self.controller = robot_controller\r\n        self.safety_thresholds = safety_thresholds or self._default_safety_thresholds()\r\n        self.emergency_stop = False\r\n        self.safety_monitor = SafetyMonitor()\r\n        \r\n        # Start safety monitoring thread\r\n        self.monitoring_thread = threading.Thread(target=self._safety_monitor_loop)\r\n        self.monitoring_thread.daemon = True\r\n        self.monitoring_thread.start()\r\n    \r\n    def _default_safety_thresholds(self):\r\n        """Define default safety thresholds"""\r\n        return {\r\n            \'velocity\': {\'max_linear\': 1.0, \'max_angular\': 0.5},  # m/s, rad/s\r\n            \'force\': {\'max_gripper\': 50.0, \'max_impact\': 10.0},   # N\r\n            \'distance\': {\'min_to_obstacle\': 0.3},                 # m\r\n            \'time\': {\'max_action_duration\': 30.0},               # seconds\r\n            \'torque\': {\'max_joint\': 100.0}                       # Nm\r\n        }\r\n    \r\n    def execute_command_safely(self, command):\r\n        """Execute a command with safety checks"""\r\n        # Parse command to action\r\n        action = self.controller.parse_command_to_action(command)\r\n        \r\n        # Validate action against safety constraints\r\n        if not self._validate_action_safety(action):\r\n            print(f"Action {action} failed safety validation")\r\n            return False\r\n        \r\n        # Execute with safety monitoring\r\n        return self._execute_with_safety_monitoring(action)\r\n    \r\n    def _validate_action_safety(self, action):\r\n        """Validate an action against safety constraints"""\r\n        # Check for emergency stop\r\n        if self.emergency_stop:\r\n            return False\r\n        \r\n        # Validate based on action type\r\n        action_type = action[\'action\']\r\n        params = action[\'params\']\r\n        \r\n        if action_type == RobotAction.MOVE_TO:\r\n            # Check if destination is safe\r\n            x, y, z = params.get(\'x\', 0), params.get(\'y\', 0), params.get(\'z\', 0)\r\n            \r\n            # This would interface with collision checking\r\n            if not self._is_path_safe(x, y, z):\r\n                return False\r\n        \r\n        elif action_type == RobotAction.GRASP:\r\n            # Check grasp force\r\n            force = params.get(\'force\', 0)\r\n            if force > self.safety_thresholds[\'force\'][\'max_gripper\']:\r\n                return False\r\n        \r\n        return True\r\n    \r\n    def _is_path_safe(self, x, y, z):\r\n        """Check if a path to (x, y, z) is safe"""\r\n        # This would interface with the navigation stack and collision checking\r\n        # For simplicity, assume it\'s implemented elsewhere\r\n        return True  # Placeholder\r\n    \r\n    def _execute_with_safety_monitoring(self, action):\r\n        """Execute an action while monitoring for safety violations"""\r\n        # Start monitoring\r\n        self.safety_monitor.start_monitoring()\r\n        \r\n        try:\r\n            # Execute the action\r\n            success = self.controller.execute_action(action)\r\n            \r\n            # Stop monitoring\r\n            self.safety_monitor.stop_monitoring()\r\n            \r\n            return success\r\n        except Exception as e:\r\n            print(f"Error executing action: {e}")\r\n            self.safety_monitor.stop_monitoring()\r\n            return False\r\n    \r\n    def _safety_monitor_loop(self):\r\n        """Background monitoring loop to check for safety violations"""\r\n        while True:\r\n            if self.emergency_stop:\r\n                self.trigger_emergency_stop()\r\n            \r\n            # Check safety conditions\r\n            if self._check_safety_violations():\r\n                self.trigger_emergency_stop()\r\n            \r\n            time.sleep(0.1)  # Check every 100ms\r\n    \r\n    def _check_safety_violations(self):\r\n        """Check for safety violations in the robot state"""\r\n        # This would check actual robot sensors and state\r\n        # For now, it returns False as a placeholder\r\n        return False\r\n    \r\n    def trigger_emergency_stop(self):\r\n        """Trigger emergency stop and halt robot"""\r\n        print("EMERGENCY STOP TRIGGERED!")\r\n        self.emergency_stop = True\r\n        # In practice, this would send an immediate stop command to the robot\r\n        self.controller.execute_action({\r\n            \'action\': RobotAction.SPEAK,\r\n            \'params\': {\'text\': \'Emergency stop activated. Stopping all operations.\'}\r\n        })\r\n\r\nclass SafetyMonitor:\r\n    def __init__(self):\r\n        self.monitoring = False\r\n        self.violations = []\r\n        \r\n    def start_monitoring(self):\r\n        """Start safety monitoring"""\r\n        self.monitoring = True\r\n        self.violations = []\r\n        print("Safety monitoring started")\r\n    \r\n    def stop_monitoring(self):\r\n        """Stop safety monitoring"""\r\n        self.monitoring = False\r\n        print(f"Safety monitoring stopped. Violations: {len(self.violations)}")\r\n    \r\n    def check_current_state(self):\r\n        """Check current robot state against safety constraints"""\r\n        if not self.monitoring:\r\n            return []\r\n        \r\n        violations = []\r\n        \r\n        # Check velocity constraints\r\n        # This would read actual velocity from robot\r\n        # violations.append("High velocity detected") if velocity > threshold\r\n        \r\n        # Check force constraints\r\n        # This would read actual force/torque from robot\r\n        # violations.append("High force detected") if force > threshold\r\n        \r\n        # Store violations\r\n        self.violations.extend(violations)\r\n        \r\n        return violations\n'})}),"\n",(0,t.jsx)(e.h2,{id:"-105-real-world-implementation-examples-",children:"\ud83d\udd28 10.5 Real-World Implementation Examples \ud83d\udd28"}),"\n",(0,t.jsx)(e.h3,{id:"-1051-voice-command-to-robot-action-pipeline-",children:"\ud83e\udd16 10.5.1 Voice Command to Robot Action Pipeline \ud83e\udd16"}),"\n",(0,t.jsx)(e.p,{children:"A complete implementation of the voice-to-action pipeline:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import speech_recognition as sr\r\nfrom transformers import AutoTokenizer, AutoModel\r\nimport torch\r\n\r\nclass VoiceToActionPipeline:\r\n    def __init__(self, robot_controller, llm_controller, safety_controller):\r\n        self.robot_controller = robot_controller\r\n        self.llm_controller = llm_controller\r\n        self.safety_controller = safety_controller\r\n        self.speech_recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        \r\n        # Initialize speech recognition settings\r\n        with self.microphone as source:\r\n            self.speech_recognizer.adjust_for_ambient_noise(source)\r\n    \r\n    def listen_and_execute(self, timeout=5):\r\n        """Listen for voice command and execute it"""\r\n        try:\r\n            print("Listening for command...")\r\n            \r\n            with self.microphone as source:\r\n                # Listen with timeout\r\n                audio = self.speech_recognizer.listen(source, timeout=timeout)\r\n            \r\n            # Recognize speech\r\n            command = self.speech_recognizer.recognize_google(audio)\r\n            print(f"Recognized command: {command}")\r\n            \r\n            # Process command with LLM to generate action plan\r\n            action_plan = self.llm_controller.plan_from_natural_language(command)\r\n            \r\n            # Validate safety of the plan\r\n            for action_step in action_plan:\r\n                if not self.safety_controller._validate_action_safety(action_step):\r\n                    print(f"Action {action_step} failed safety validation")\r\n                    return False\r\n            \r\n            # Execute the plan\r\n            success = self.llm_controller.execute_plan(action_plan)\r\n            \r\n            return success\r\n            \r\n        except sr.WaitTimeoutError:\r\n            print("No speech detected within timeout period")\r\n            return False\r\n        except sr.UnknownValueError:\r\n            print("Could not understand the audio")\r\n            return False\r\n        except sr.RequestError as e:\r\n            print(f"Error with speech recognition service: {e}")\r\n            return False\r\n        except Exception as e:\r\n            print(f"Unexpected error: {e}")\r\n            return False\r\n\r\n    def continuous_listening_mode(self):\r\n        """Run in continuous listening mode"""\r\n        print("Starting continuous listening mode. Press Ctrl+C to stop.")\r\n        \r\n        try:\r\n            while True:\r\n                success = self.listen_and_execute()\r\n                \r\n                if success:\r\n                    print("Command executed successfully")\r\n                else:\r\n                    print("Command execution failed or was cancelled")\r\n                \r\n                # Small pause between commands\r\n                time.sleep(0.5)\r\n                \r\n        except KeyboardInterrupt:\r\n            print("\\nContinuous listening stopped by user")\r\n\r\n# \u2139\ufe0f Example usage of the complete VLA system \u2139\ufe0f\r\ndef run_complete_vla_example():\r\n    """Run a complete example of VLA integration"""\r\n    print("Initializing Vision-Language-Action Integration System...")\r\n    \r\n    # Initialize robot interface\r\n    robot_interface = RobotInterface()\r\n    \r\n    # Initialize language-to-action mapper\r\n    language_mapper = LanguageToActionMapper(robot_interface)\r\n    \r\n    # Initialize LLM controller for complex task planning\r\n    llm_controller = LLMRobotController(robot_action_space=language_mapper.action_space)\r\n    \r\n    # Initialize safety controller\r\n    safety_controller = SafeVLAController(language_mapper)\r\n    \r\n    # Initialize the complete VLA pipeline\r\n    vla_pipeline = VoiceToActionPipeline(\r\n        robot_controller=language_mapper,\r\n        llm_controller=llm_controller,\r\n        safety_controller=safety_controller\r\n    )\r\n    \r\n    # Test with various commands\r\n    test_commands = [\r\n        "Please go to the kitchen and bring me the water bottle",\r\n        "Move to the living room",\r\n        "Pick up the red cup from the table",\r\n        "Navigate to the charging station"\r\n    ]\r\n    \r\n    for command in test_commands:\r\n        print(f"\\nTesting command: \'{command}\'")\r\n        \r\n        # Plan and execute with LLM controller\r\n        action_plan = llm_controller.plan_from_natural_language(command)\r\n        print(f"Generated action plan: {action_plan}")\r\n        \r\n        success = llm_controller.execute_plan(action_plan)\r\n        print(f"Command \'{command}\' execution: {\'SUCCESS\' if success else \'FAILED\'}")\r\n    \r\n    # Uncomment to run continuous listening mode\r\n    # vla_pipeline.continuous_listening_mode()\r\n\r\n# \u2139\ufe0f Run the example \u2139\ufe0f\r\nif __name__ == "__main__":\r\n    run_complete_vla_example()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"\ufe0f-1052-vision-guided-manipulation-with-language-understanding-\ufe0f",children:"\ud83d\udc41\ufe0f 10.5.2 Vision-Guided Manipulation with Language Understanding \ud83d\udc41\ufe0f"}),"\n",(0,t.jsx)(e.p,{children:"Combining visual perception and language for complex manipulation tasks:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class VisionGuidedManipulation:\r\n    def __init__(self, vision_module, manipulation_controller, language_understanding):\r\n        self.vision = vision_module\r\n        self.manipulation = manipulation_controller\r\n        self.language = language_understanding\r\n        self.object_memory = {}  # Track objects seen and their properties\r\n    \r\n    def process_vision_language_command(self, image, command):\r\n        \"\"\"Process a command that requires both vision and language understanding\"\"\"\r\n        # Step 1: Analyze the scene\r\n        detected_objects = self.vision.get_objects_with_descriptions(\r\n            image, \r\n            class_names=[\"person\", \"bottle\", \"cup\", \"book\", \"phone\", \"box\", \"table\", \"chair\"]\r\n        )\r\n        \r\n        # Step 2: Understand the command\r\n        command_parsed = self.language.parse_command(command)\r\n        \r\n        # Step 3: Match command to detected objects\r\n        target_object = self._find_target_object(command_parsed, detected_objects)\r\n        \r\n        if not target_object:\r\n            return {\r\n                'success': False,\r\n                'message': f'Could not find target object: {command_parsed.get(\"objects\", [])}'\r\n            }\r\n        \r\n        # Step 4: Plan the manipulation action\r\n        action = self._plan_manipulation_action(command_parsed, target_object)\r\n        \r\n        # Step 5: Execute the action\r\n        success = self.manipulation.execute_action(action)\r\n        \r\n        return {\r\n            'success': success,\r\n            'action': action,\r\n            'target_object': target_object\r\n        }\r\n    \r\n    def _find_target_object(self, command_parsed, detected_objects):\r\n        \"\"\"Find the target object based on the command\"\"\"\r\n        # Look for object types mentioned in the command\r\n        command_objects = command_parsed.get('objects', [])\r\n        target_obj = None\r\n        \r\n        for obj in detected_objects:\r\n            obj_name = obj['name'].lower()\r\n            \r\n            # Check if this object matches the command\r\n            for cmd_obj in command_objects:\r\n                if cmd_obj in obj_name or obj_name in cmd_obj:\r\n                    target_obj = obj\r\n                    break\r\n            \r\n            if target_obj:\r\n                break\r\n        \r\n        return target_obj\r\n    \r\n    def _plan_manipulation_action(self, command_parsed, target_object):\r\n        \"\"\"Plan the manipulation action based on command and target object\"\"\"\r\n        command_action = command_parsed.get('action')\r\n        object_bbox = target_object['bbox']\r\n        \r\n        # Calculate the center of the bounding box\r\n        center_x = (object_bbox[0] + object_bbox[2]) / 2\r\n        center_y = (object_bbox[1] + object_bbox[3]) / 2\r\n        \r\n        if command_action == 'grasp':\r\n            return {\r\n                'action': RobotAction.GRASP,\r\n                'params': {\r\n                    'object_id': target_object['name'],\r\n                    'x': center_x,\r\n                    'y': center_y,\r\n                    'confidence': target_object['confidence']\r\n                }\r\n            }\r\n        elif command_action == 'inspect':\r\n            return {\r\n                'action': RobotAction.MOVE_TO,\r\n                'params': {\r\n                    'x': center_x,\r\n                    'y': center_y,\r\n                    'z': 0.0,\r\n                    'orientation': 0.0\r\n                }\r\n            }\r\n        else:\r\n            # Default to speaking if action is not recognized\r\n            return {\r\n                'action': RobotAction.SPEAK,\r\n                'params': {'text': f\"I'm not sure how to {command_action} the {target_object['name']}.\"}\r\n            }\r\n\r\n# \ud83d\udc41\ufe0f Example of using Vision-Guided Manipulation \ud83d\udc41\ufe0f\r\ndef run_vision_guided_manipulation_example():\r\n    \"\"\"Run an example of vision-guided manipulation\"\"\"\r\n    print(\"Running Vision-Guided Manipulation Example...\")\r\n    \r\n    # Initialize components\r\n    vision_module = ObjectDetectionModule()\r\n    robot_interface = RobotInterface()\r\n    language_understanding = RobotCommandProcessor()\r\n    manipulation_controller = LanguageToActionMapper(robot_interface)\r\n    \r\n    # Create the vision-guided system\r\n    vgm = VisionGuidedManipulation(\r\n        vision_module=vision_module,\r\n        manipulation_controller=manipulation_controller,\r\n        language_understanding=language_understanding\r\n    )\r\n    \r\n    # Simulate an image (in practice, this would come from robot's camera)\r\n    # For this example, we'll use a dummy tensor\r\n    dummy_image = torch.rand(1, 3, 224, 224)  # Batch of 1, 3 channels, 224x224\r\n    \r\n    # Test commands\r\n    commands = [\r\n        \"grasp the bottle\",\r\n        \"pick up the cup\",\r\n        \"look at the book\"\r\n    ]\r\n    \r\n    for command in commands:\r\n        print(f\"\\nProcessing command: '{command}'\")\r\n        \r\n        result = vgm.process_vision_language_command(dummy_image, command)\r\n        \r\n        print(f\"Result: {result['success']}\")\r\n        if 'action' in result:\r\n            print(f\"Action planned: {result['action']}\")\r\n        if 'message' in result:\r\n            print(f\"Message: {result['message']}\")\r\n\r\n# \u2139\ufe0f Run the example \u2139\ufe0f\r\nif __name__ == \"__main__\":\r\n    run_vision_guided_manipulation_example()\n"})}),"\n",(0,t.jsx)(e.h2,{id:"-106-evaluation-and-assessment-",children:"\ud83d\udcca 10.6 Evaluation and Assessment \ud83d\udcca"}),"\n",(0,t.jsx)(e.h3,{id:"-1061-vla-system-performance-metrics-",children:"\ud83d\udcc8 10.6.1 VLA System Performance Metrics \ud83d\udcc8"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class VLAEvaluator:\r\n    def __init__(self):\r\n        self.completion_rate = 0\r\n        self.accuracy_rate = 0\r\n        self.response_time_avg = 0\r\n        self.safety_violations = 0\r\n        self.natural_language_understanding_score = 0\r\n        \r\n    def evaluate_system(self, test_commands, expected_outcomes):\r\n        """Evaluate the VLA system on a set of test commands"""\r\n        total_commands = len(test_commands)\r\n        successful_completions = 0\r\n        accurate_executions = 0\r\n        total_response_time = 0\r\n        safety_violations = 0\r\n        \r\n        for i, command in enumerate(test_commands):\r\n            start_time = time.time()\r\n            \r\n            # Execute command\r\n            result = self.execute_command(command)\r\n            \r\n            response_time = time.time() - start_time\r\n            total_response_time += response_time\r\n            \r\n            # Check if command completed successfully\r\n            if result[\'success\']:\r\n                successful_completions += 1\r\n                \r\n                # Check if executed correctly according to expected outcome\r\n                if self.check_outcome_correctness(result, expected_outcomes[i]):\r\n                    accurate_executions += 1\r\n            \r\n            # Check for safety violations (this is a placeholder check)\r\n            if result.get(\'safety_violation\', False):\r\n                safety_violations += 1\r\n        \r\n        # Calculate metrics\r\n        self.completion_rate = successful_completions / total_commands if total_commands > 0 else 0\r\n        self.accuracy_rate = accurate_executions / total_commands if total_commands > 0 else 0\r\n        self.response_time_avg = total_response_time / total_commands if total_commands > 0 else 0\r\n        self.safety_violations = safety_violations\r\n        \r\n        return {\r\n            \'completion_rate\': self.completion_rate,\r\n            \'accuracy_rate\': self.accuracy_rate,\r\n            \'avg_response_time\': self.response_time_avg,\r\n            \'safety_violations\': self.safety_violations\r\n        }\r\n    \r\n    def execute_command(self, command):\r\n        """Execute a command (placeholder implementation)"""\r\n        # This would connect to the actual VLA system\r\n        # For this example, we\'ll simulate execution\r\n        import random\r\n        \r\n        # Simulate different outcomes based on command complexity\r\n        success = random.random() > 0.2  # 80% success rate for simulation\r\n        response_time = random.uniform(0.5, 3.0)  # Random response time\r\n        \r\n        # Simulate safety check (rare violations)\r\n        safety_violation = random.random() < 0.05  # 5% of actions have safety violations\r\n        \r\n        return {\r\n            \'success\': success,\r\n            \'safety_violation\': safety_violation,\r\n            \'execution_details\': f\'Simulated execution of: {command}\'\r\n        }\r\n    \r\n    def check_outcome_correctness(self, result, expected_outcome):\r\n        """Check if the execution result matches the expected outcome"""\r\n        # This would involve comparing the actual robot state to expected state\r\n        # For this example, we\'ll simulate correctness\r\n        import random\r\n        return random.random() > 0.3  # 70% of completed tasks are considered correct\r\n    \r\n    def generate_performance_report(self):\r\n        """Generate a comprehensive performance report"""\r\n        report = f"""\r\nVLA System Performance Report\r\n=============================\r\n\r\nTask Completion: {self.completion_rate:.2%}\r\nExecution Accuracy: {self.accuracy_rate:.2%}\r\nAverage Response Time: {self.response_time_avg:.2f} seconds\r\nSafety Violations: {self.safety_violations}\r\n\r\nRecommendations:\r\n- {\'Improve natural language understanding\' if self.completion_rate < 0.8 else \'NL understanding is adequate\'}\r\n- {\'Optimize action execution pipeline\' if self.accuracy_rate < 0.8 else \'Action execution is accurate\'}\r\n- {\'Optimize response time\' if self.response_time_avg > 2.0 else \'Response time is acceptable\'}\r\n- {\'Review safety protocols\' if self.safety_violations > 0 else \'Safety performance is good\'}\r\n        """\r\n        return report\r\n\r\n# \ud83d\udcca Example evaluation \ud83d\udcca\r\ndef run_vla_evaluation():\r\n    """Run an evaluation of the VLA system"""\r\n    evaluator = VLAEvaluator()\r\n    \r\n    # Define test commands and expected outcomes\r\n    test_commands = [\r\n        "Move to the kitchen counter",\r\n        "Pick up the red cup",\r\n        "Place the cup on the table",\r\n        "Navigate to the living room",\r\n        "Grasp the book",\r\n        "Go to the charging station",\r\n        "Find the water bottle",\r\n        "Take the phone",\r\n        "Put the object down",\r\n        "Move toward the door"\r\n    ]\r\n    \r\n    expected_outcomes = [None] * len(test_commands)  # Placeholder outcomes\r\n    \r\n    # Evaluate the system\r\n    results = evaluator.evaluate_system(test_commands, expected_outcomes)\r\n    \r\n    print("VLA System Evaluation Results:")\r\n    print(f"Completion Rate: {results[\'completion_rate\']:.2%}")\r\n    print(f"Accuracy Rate: {results[\'accuracy_rate\']:.2%}")\r\n    print(f"Average Response Time: {results[\'avg_response_time\']:.2f}s")\r\n    print(f"Safety Violations: {results[\'safety_violations\']}")\r\n    \r\n    print("\\n" + evaluator.generate_performance_report())\r\n\r\n# \ud83d\udcca Run the evaluation \ud83d\udcca\r\nif __name__ == "__main__":\r\n    run_vla_evaluation()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"-107-summary-",children:"\ud83d\udcdd 10.7 Summary \ud83d\udcdd"}),"\n",(0,t.jsx)(e.p,{children:"Vision-Language-Action (VLA) integration is a crucial component of modern physical AI systems, enabling robots to understand and respond to complex human commands in natural environments. This chapter covered:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"VLA Framework"}),": Understanding how visual perception, language processing, and action execution work together."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Vision Processing"}),": Implementing object detection, scene understanding, and visual goal-conditioned policies for robotics."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Language Processing"}),": Incorporating natural language understanding and large language model integration for command interpretation."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Action Execution"}),": Designing mappings between language concepts and robot actions, with hierarchical task planning."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Safety Considerations"}),": Implementing safety checks and validation for language-guided robot actions."]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Integration Examples"}),": Complete implementations of voice-to-action pipelines and vision-guided manipulation."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The VLA approach enables robots to operate in human environments using natural communication modalities, significantly expanding their usability and effectiveness."}),"\n",(0,t.jsx)(e.h3,{id:"\u2139\ufe0f-key-takeaways-\u2139\ufe0f",children:"\u2139\ufe0f Key Takeaways: \u2139\ufe0f"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"VLA systems combine computer vision, NLP, and robotics to create intuitive human-robot interfaces"}),"\n",(0,t.jsx)(e.li,{children:"Large language models can be used for high-level task planning and command interpretation"}),"\n",(0,t.jsx)(e.li,{children:"Safety validation is critical when executing language-guided actions"}),"\n",(0,t.jsx)(e.li,{children:"Hierarchical task planning breaks complex commands into executable primitive actions"}),"\n",(0,t.jsx)(e.li,{children:"Evaluation of VLA systems requires metrics for completion rate, accuracy, and response time"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"-knowledge-check-",children:"\ud83e\udd14 Knowledge Check \ud83e\udd14"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Explain the key components of a Vision-Language-Action (VLA) system."}),"\n",(0,t.jsx)(e.li,{children:"How do large language models enhance robot task planning?"}),"\n",(0,t.jsx)(e.li,{children:"What safety considerations are necessary when executing language-guided robot actions?"}),"\n",(0,t.jsx)(e.li,{children:"Describe the process of mapping natural language commands to robot actions."}),"\n",(0,t.jsx)(e.li,{children:"What metrics are important for evaluating VLA system performance?"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsxs)(e.em,{children:["Continue to ",(0,t.jsx)(e.a,{href:"./chapter-11-advanced-humanoid-control.md",children:"Chapter 11: Advanced Humanoid Control"})]})})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);