"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[741],{647:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});var o=e(4848),a=e(8453);const t={},s="\ud83e\udd16 \ud83e\udd16 Curriculum Module: Physical AI & Humanoid Robotics \ud83e\udde0 \ud83e\udd16",r={id:"introduction",title:"\ud83e\udd16 \ud83e\udd16 Curriculum Module: Physical AI & Humanoid Robotics \ud83e\udde0 \ud83e\udd16",description:"Welcome to the comprehensive guide on Physical AI & Humanoid Robotics. This book covers everything from the fundamentals of embodied intelligence to advanced humanoid control systems, following the principles of applying AI knowledge to control Humanoid Robots in simulated and real-world environments.",source:"@site/docusaurus/docs/introduction.md",sourceDirName:".",slug:"/introduction",permalink:"/Humanoid-Robotic-Book/docs/introduction",draft:!1,unlisted:!1,editUrl:"https://github.com/aamna847/Humanoid-Robotic-Book/edit/main/docusaurus/docs/introduction.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",next:{title:"Chapter 1 - Introduction to Physical AI & Embodied Intelligence",permalink:"/Humanoid-Robotic-Book/docs/part-1-foundations/chapter-1-introduction-to-physical-ai"}},l={},c=[{value:"\ud83d\udccb \ud83d\udcda Table of Contents \ud83d\udccb",id:"--table-of-contents-",level:2},{value:"\ud83d\udcca \ud83c\udf93 Course Overview \ud83d\udcca",id:"--course-overview-",level:2},{value:"\ud83e\udd16 \ud83e\udde0 Physical AI Principles \ud83e\udd16",id:"--physical-ai-principles-",level:2},{value:"\ud83e\udd16 \ud83e\udd16 Humanoid Robotics Focus \ud83e\udd16",id:"--humanoid-robotics-focus-",level:2},{value:"\ud83e\udd16 \u26a1 Module 1: The Robotic Nervous System (ROS 2) \ud83e\udd16",id:"--module-1-the-robotic-nervous-system-ros-2-",level:3},{value:"\ud83c\udfae \ud83c\udfae Module 2: The Digital Twin (Gazebo &amp; Unity) \ud83c\udfae",id:"--module-2-the-digital-twin-gazebo--unity-",level:3},{value:"\ud83e\udd16 \ud83d\ude80 Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122) \ud83e\udd16",id:"--module-3-the-ai-robot-brain-nvidia-isaac-",level:3},{value:"\ud83d\udc41\ufe0f \ud83d\udcac Module 4: Vision-Language-Action (VLA) \ud83d\udc41\ufe0f",id:"\ufe0f--module-4-vision-language-action-vla-\ufe0f",level:3},{value:"\ud83e\udd16 \ud83d\ude80 Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122) \ud83e\udd16",id:"--module-3-the-ai-robot-brain-nvidia-isaac--1",level:3},{value:"\ud83d\udc41\ufe0f \ud83d\udcac Module 4: Vision-Language-Action (VLA) \ud83d\udc41\ufe0f",id:"\ufe0f--module-4-vision-language-action-vla-\ufe0f-1",level:3},{value:"\ud83e\udd16 \ud83c\udf0d Why Physical AI Matters \ud83e\udd16",id:"--why-physical-ai-matters-",level:2},{value:"\ud83c\udfaf \ud83c\udfaf Learning Outcomes \ud83c\udfaf",id:"--learning-outcomes-",level:2}];function d(n){const i={a:"a",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(i.h1,{id:"--curriculum-module-physical-ai--humanoid-robotics--",children:"\ud83e\udd16 \ud83e\udd16 Curriculum Module: Physical AI & Humanoid Robotics \ud83e\udde0 \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Welcome to the comprehensive guide on Physical AI & Humanoid Robotics. This book covers everything from the fundamentals of embodied intelligence to advanced humanoid control systems, following the principles of applying AI knowledge to control Humanoid Robots in simulated and real-world environments."}),"\n",(0,o.jsx)(i.h2,{id:"--table-of-contents-",children:"\ud83d\udccb \ud83d\udcda Table of Contents \ud83d\udccb"}),"\n",(0,o.jsxs)(i.ol,{children:["\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-1-foundations/chapter-1-introduction-to-physical-ai",children:"Part 1: Foundations"})}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Chapter 1: Introduction to Physical AI"}),"\n",(0,o.jsx)(i.li,{children:"Chapter 2: Sensors & Perception"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-2-nervous-system/chapter-3-ros2-architecture-core-concepts",children:"Part 2: The Robotic Nervous System"})}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Chapter 3: ROS 2 Architecture & Core Concepts"}),"\n",(0,o.jsx)(i.li,{children:"Chapter 4: Building ROS 2 Nodes in Python"}),"\n",(0,o.jsx)(i.li,{children:"Chapter 5: Launch Systems & Parameter Management"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"./part-3-digital-twin/chapter-5-simulation-gazebo-unity-integration.md",children:"Part 3: The Digital Twin"})}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Chapter 5: Simulation - Gazebo & Unity Integration"}),"\n",(0,o.jsx)(i.li,{children:"Chapter 6: NVIDIA Isaac Sim SDK"}),"\n",(0,o.jsx)(i.li,{children:"Chapter 7: Digital Twin Environment Design"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"./part-4-ai-brain/chapter-8-human-in-the-loop-teleoperation.md",children:"Part 4: The AI-Brain"})}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Chapter 8: Human-in-the-Loop Teleoperation"}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-9-visual-slam-navigation",children:"Chapter 9: Robot Learning & AI Integration"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-10-reinforcement-learning-control",children:"Chapter 10: Vision-Language-Action Integration"})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"./part-4-interaction/chapter-8-human-in-the-loop-teleoperation.md",children:"Part 4: Interaction"})}),"\n"]}),"\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-9-visual-slam-navigation",children:"Part 4: AI Brain"})}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-9-visual-slam-navigation",children:"Chapter 9: Robot Learning & AI Integration"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-10-reinforcement-learning-control",children:"Chapter 10: Vision-Language-Action Integration"})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(i.li,{children:["\n",(0,o.jsx)(i.p,{children:(0,o.jsx)(i.a,{href:"./part-5-advanced-humanoids/",children:"Part 5: Advanced Humanoids"})}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"./part-4-ai-brain/chapter-11-advanced-humanoid-control.md",children:"Chapter 11: Advanced Humanoid Control"})}),"\n",(0,o.jsx)(i.li,{children:(0,o.jsx)(i.a,{href:"./part-4-ai-brain/chapter-12-capstone-autonomous-humanoid.md",children:"Chapter 12: Capstone - Autonomous Humanoid"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"--course-overview-",children:"\ud83d\udcca \ud83c\udf93 Course Overview \ud83d\udcca"}),"\n",(0,o.jsx)(i.p,{children:"The future of AI extends beyond digital spaces into the physical world. This capstone quarter introduces Physical AI\u2014AI systems that function in reality and comprehend physical laws. Students learn to design, simulate, and deploy humanoid robots capable of natural human interactions using ROS 2, Gazebo, and NVIDIA Isaac."}),"\n",(0,o.jsx)(i.h2,{id:"--physical-ai-principles-",children:"\ud83e\udd16 \ud83e\udde0 Physical AI Principles \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Physical AI represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space. This approach focuses on:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Understanding physical laws and their impact on AI systems"}),"\n",(0,o.jsx)(i.li,{children:"Developing embodied intelligence that interacts with the real world"}),"\n",(0,o.jsx)(i.li,{children:"Creating AI systems that function in reality rather than just simulations"}),"\n",(0,o.jsx)(i.li,{children:"Integrating real-world sensorimotor experiences into AI models"}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"--humanoid-robotics-focus-",children:"\ud83e\udd16 \ud83e\udd16 Humanoid Robotics Focus \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Our curriculum specifically emphasizes humanoid robotics because:"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Humanoid robots excel in human-centered environments due to their shared physical form"}),"\n",(0,o.jsx)(i.li,{children:"They can be trained with abundant data from human interaction environments"}),"\n",(0,o.jsx)(i.li,{children:"They represent the pinnacle of embodied intelligence challenges"}),"\n",(0,o.jsx)(i.li,{children:"They require integration of multiple complex systems (locomotion, manipulation, perception)"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"--module-1-the-robotic-nervous-system-ros-2-",children:"\ud83e\udd16 \u26a1 Module 1: The Robotic Nervous System (ROS 2) \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Focus: Middleware for robot control."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"ROS 2 Nodes, Topics, and Services"}),"\n",(0,o.jsx)(i.li,{children:"Bridging Python Agents to ROS controllers using rclpy"}),"\n",(0,o.jsx)(i.li,{children:"Understanding URDF (Unified Robot Description Format) for humanoids"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"--module-2-the-digital-twin-gazebo--unity-",children:"\ud83c\udfae \ud83c\udfae Module 2: The Digital Twin (Gazebo & Unity) \ud83c\udfae"}),"\n",(0,o.jsx)(i.p,{children:"Focus: Physics simulation and environment building."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Simulating physics, gravity, and collisions in Gazebo"}),"\n",(0,o.jsx)(i.li,{children:"High-fidelity rendering and human-robot interaction in Unity"}),"\n",(0,o.jsx)(i.li,{children:"Simulating sensors: LiDAR, Depth Cameras, and IMUs"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"--module-3-the-ai-robot-brain-nvidia-isaac-",children:"\ud83e\udd16 \ud83d\ude80 Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122) \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Focus: Advanced perception and training."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"NVIDIA Isaac Sim: Photorealistic simulation and synthetic data generation"}),"\n",(0,o.jsx)(i.li,{children:"Isaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation"}),"\n",(0,o.jsx)(i.li,{children:"Nav2: Path planning for bipedal humanoid movement"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"\ufe0f--module-4-vision-language-action-vla-\ufe0f",children:"\ud83d\udc41\ufe0f \ud83d\udcac Module 4: Vision-Language-Action (VLA) \ud83d\udc41\ufe0f"}),"\n",(0,o.jsx)(i.p,{children:"Focus: The convergence of LLMs and Robotics."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Voice-to-Action: Using OpenAI Whisper for voice commands"}),"\n",(0,o.jsx)(i.li,{children:'Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions'}),"\n",(0,o.jsx)(i.li,{children:"Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it."}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"--module-3-the-ai-robot-brain-nvidia-isaac--1",children:"\ud83e\udd16 \ud83d\ude80 Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122) \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Focus: Advanced perception and training."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"NVIDIA Isaac Sim: Photorealistic simulation and synthetic data generation"}),"\n",(0,o.jsx)(i.li,{children:"Isaac ROS: Hardware-accelerated VSLAM (Visual SLAM) and navigation"}),"\n",(0,o.jsx)(i.li,{children:"Nav2: Path planning for bipedal humanoid movement"}),"\n"]}),"\n",(0,o.jsx)(i.h3,{id:"\ufe0f--module-4-vision-language-action-vla-\ufe0f-1",children:"\ud83d\udc41\ufe0f \ud83d\udcac Module 4: Vision-Language-Action (VLA) \ud83d\udc41\ufe0f"}),"\n",(0,o.jsx)(i.p,{children:"Focus: The convergence of LLMs and Robotics."}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Voice-to-Action: Using OpenAI Whisper for voice commands"}),"\n",(0,o.jsx)(i.li,{children:'Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions'}),"\n",(0,o.jsx)(i.li,{children:"Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it."}),"\n"]}),"\n",(0,o.jsx)(i.h2,{id:"--why-physical-ai-matters-",children:"\ud83e\udd16 \ud83c\udf0d Why Physical AI Matters \ud83e\udd16"}),"\n",(0,o.jsx)(i.p,{children:"Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space."}),"\n",(0,o.jsx)(i.h2,{id:"--learning-outcomes-",children:"\ud83c\udfaf \ud83c\udfaf Learning Outcomes \ud83c\udfaf"}),"\n",(0,o.jsxs)(i.ul,{children:["\n",(0,o.jsx)(i.li,{children:"Understand Physical AI principles and embodied intelligence"}),"\n",(0,o.jsx)(i.li,{children:"Master ROS 2 (Robot Operating System) for robotic control"}),"\n",(0,o.jsx)(i.li,{children:"Simulate robots with Gazebo and Unity"}),"\n",(0,o.jsx)(i.li,{children:"Develop with NVIDIA Isaac AI robot platform"}),"\n",(0,o.jsx)(i.li,{children:"Design humanoid robots for natural interactions"}),"\n",(0,o.jsx)(i.li,{children:"Integrate GPT models for conversational robotics"}),"\n"]})]})}function h(n={}){const{wrapper:i}={...(0,a.R)(),...n.components};return i?(0,o.jsx)(i,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>s,x:()=>r});var o=e(6540);const a={},t=o.createContext(a);function s(n){const i=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function r(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),o.createElement(t.Provider,{value:i},n.children)}}}]);