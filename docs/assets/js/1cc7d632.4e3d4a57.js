"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4312],{8127:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var t=r(4848),i=r(8453);const a={slug:"chapter-9-visual-slam-navigation",title:"Chapter 9 - Visual SLAM & Navigation (Nav2)",description:"Comprehensive guide to Visual SLAM and navigation using Nav2 for robotics",tags:["slam","visual-slam","navigation","nav2","robotics","ai"]},s="\ud83d\udcda Chapter 9: Visual SLAM & Navigation (Nav2) \ud83d\udcda",o={id:"part-4-ai-brain/chapter-9-robot-learning-ai-integration",title:"Chapter 9 - Visual SLAM & Navigation (Nav2)",description:"Comprehensive guide to Visual SLAM and navigation using Nav2 for robotics",source:"@site/docusaurus/docs/part-4-ai-brain/chapter-9-robot-learning-ai-integration.md",sourceDirName:"part-4-ai-brain",slug:"/part-4-ai-brain/chapter-9-visual-slam-navigation",permalink:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-9-visual-slam-navigation",draft:!1,unlisted:!1,editUrl:"https://github.com/aamna847/Humanoid-Robotic-Book/edit/main/docusaurus/docs/part-4-ai-brain/chapter-9-robot-learning-ai-integration.md",tags:[{label:"slam",permalink:"/Humanoid-Robotic-Book/docs/tags/slam"},{label:"visual-slam",permalink:"/Humanoid-Robotic-Book/docs/tags/visual-slam"},{label:"navigation",permalink:"/Humanoid-Robotic-Book/docs/tags/navigation"},{label:"nav2",permalink:"/Humanoid-Robotic-Book/docs/tags/nav-2"},{label:"robotics",permalink:"/Humanoid-Robotic-Book/docs/tags/robotics"},{label:"ai",permalink:"/Humanoid-Robotic-Book/docs/tags/ai"}],version:"current",frontMatter:{slug:"chapter-9-visual-slam-navigation",title:"Chapter 9 - Visual SLAM & Navigation (Nav2)",description:"Comprehensive guide to Visual SLAM and navigation using Nav2 for robotics",tags:["slam","visual-slam","navigation","nav2","robotics","ai"]},sidebar:"tutorialSidebar",previous:{title:"Chapter 6 - NVIDIA Isaac Sim & SDK",permalink:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-6-nvidia-isaac-sim-sdk"},next:{title:"Chapter 10 - Reinforcement Learning for Control",permalink:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-10-reinforcement-learning-control"}},l={},d=[{value:"\ud83c\udfaf Learning Objectives \ud83c\udfaf",id:"-learning-objectives-",level:2},{value:"\ud83d\udc4b 9.1 Introduction to Robot Learning \ud83d\udc4b",id:"-91-introduction-to-robot-learning-",level:2},{value:"\ud83c\udfaf 9.1.1 Types of Robot Learning \ud83c\udfaf",id:"-911-types-of-robot-learning-",level:3},{value:"\ud83c\udfaf 9.1.2 Challenges in Robot Learning \ud83c\udfaf",id:"-912-challenges-in-robot-learning-",level:3},{value:"\ud83c\udfaf 9.2 Perception Systems with Deep Learning \ud83c\udfaf",id:"-92-perception-systems-with-deep-learning-",level:2},{value:"\ud83d\udc41\ufe0f 9.2.1 Convolutional Neural Networks for Vision \ud83d\udc41\ufe0f",id:"\ufe0f-921-convolutional-neural-networks-for-vision-\ufe0f",level:3},{value:"\u2139\ufe0f 9.2.2 Semantic Segmentation for Scene Understanding \u2139\ufe0f",id:"\u2139\ufe0f-922-semantic-segmentation-for-scene-understanding-\u2139\ufe0f",level:3},{value:"\u2139\ufe0f 9.2.3 Depth Estimation \u2139\ufe0f",id:"\u2139\ufe0f-923-depth-estimation-\u2139\ufe0f",level:3},{value:"\ud83c\udfaf 9.3 Robot Control with Supervised Learning \ud83c\udfaf",id:"-93-robot-control-with-supervised-learning-",level:2},{value:"\u26a1 9.3.1 Sensor-to-Action Mapping \u26a1",id:"-931-sensor-to-action-mapping-",level:3},{value:"\ud83c\udf9b\ufe0f 9.3.2 PID Controllers Enhanced with ML \ud83c\udf9b\ufe0f",id:"\ufe0f-932-pid-controllers-enhanced-with-ml-\ufe0f",level:3},{value:"\ud83c\udfaf 9.4 Deep Learning Architectures for Robotics \ud83c\udfaf",id:"-94-deep-learning-architectures-for-robotics-",level:2},{value:"\u2139\ufe0f 9.4.1 Recurrent Neural Networks for Sequential Decision Making \u2139\ufe0f",id:"\u2139\ufe0f-941-recurrent-neural-networks-for-sequential-decision-making-\u2139\ufe0f",level:3},{value:"\ud83e\udde0 9.4.2 Attention Mechanisms for Multi-Modal Processing \ud83e\udde0",id:"-942-attention-mechanisms-for-multi-modal-processing-",level:3},{value:"\ud83e\udde0 9.4.3 Convolutional LSTM for Spatiotemporal Processing \ud83e\udde0",id:"-943-convolutional-lstm-for-spatiotemporal-processing-",level:3},{value:"\ud83c\udfaf 9.5 Reinforcement Learning for Robotics \ud83c\udfaf",id:"-95-reinforcement-learning-for-robotics-",level:2},{value:"\u26a1 9.5.1 Deep Q-Network (DQN) for Discrete Actions \u26a1",id:"-951-deep-q-network-dqn-for-discrete-actions-",level:3},{value:"\u26a1 9.5.2 Policy Gradient Methods (PPO) for Continuous Actions \u26a1",id:"-952-policy-gradient-methods-ppo-for-continuous-actions-",level:3},{value:"\ud83c\udfaf 9.5.3 Soft Actor-Critic (SAC) for Sample Efficient Learning \ud83c\udfaf",id:"-953-soft-actor-critic-sac-for-sample-efficient-learning-",level:3},{value:"\ud83e\udd16 9.6 Sensor Fusion with AI \ud83e\udd16",id:"-96-sensor-fusion-with-ai-",level:2},{value:"\ud83d\udd28 9.6.1 Kalman Filter Implementation \ud83d\udd28",id:"-961-kalman-filter-implementation-",level:3},{value:"\ud83d\udd28 9.6.2 Particle Filter Implementation \ud83d\udd28",id:"-962-particle-filter-implementation-",level:3},{value:"\ud83d\udd17 9.6.3 Neural Network-Based Sensor Fusion \ud83d\udd17",id:"-963-neural-network-based-sensor-fusion-",level:3},{value:"\ud83c\udfaf 9.7 Learning from Demonstrations \ud83c\udfaf",id:"-97-learning-from-demonstrations-",level:2},{value:"\u2139\ufe0f 9.7.1 Behavior Cloning \u2139\ufe0f",id:"\u2139\ufe0f-971-behavior-cloning-\u2139\ufe0f",level:3},{value:"\ud83d\udcca 9.7.2 DAgger (Dataset Aggregation) \ud83d\udcca",id:"-972-dagger-dataset-aggregation-",level:3},{value:"\ud83c\udfaf 9.7.3 Generative Adversarial Imitation Learning (GAIL) \ud83c\udfaf",id:"-973-generative-adversarial-imitation-learning-gail-",level:3},{value:"\ud83e\udd16 9.8 Navigation and Path Planning with AI \ud83e\udd16",id:"-98-navigation-and-path-planning-with-ai-",level:2},{value:"\u2139\ufe0f 9.8.1 A* Algorithm with Neural Heuristics \u2139\ufe0f",id:"\u2139\ufe0f-981-a-algorithm-with-neural-heuristics-\u2139\ufe0f",level:3},{value:"\ud83c\udfaf 9.8.2 Learning-based Path Planning \ud83c\udfaf",id:"-982-learning-based-path-planning-",level:3},{value:"\ud83c\udfaf 9.8.3 RRT* with Learning Enhancement \ud83c\udfaf",id:"-983-rrt-with-learning-enhancement-",level:3},{value:"\ud83c\udfaf 9.9 Human-Robot Interaction Learning \ud83c\udfaf",id:"-99-human-robot-interaction-learning-",level:2},{value:"\ud83c\udfaf 9.9.1 Learning from Human Feedback (LfHF) \ud83c\udfaf",id:"-991-learning-from-human-feedback-lfhf-",level:3},{value:"\ud83c\udfaf 9.9.2 Interactive Robot Learning \ud83c\udfaf",id:"-992-interactive-robot-learning-",level:3},{value:"\ud83d\udcca 9.10 Model Evaluation and Validation \ud83d\udcca",id:"-910-model-evaluation-and-validation-",level:2},{value:"\ud83c\udfaf 9.10.1 Metrics for Robot Learning \ud83c\udfaf",id:"-9101-metrics-for-robot-learning-",level:3},{value:"\ud83c\udfae 9.10.2 Simulation-to-Real Transfer Validation \ud83c\udfae",id:"-9102-simulation-to-real-transfer-validation-",level:3},{value:"\ud83d\ude9a 9.11 Deployment Considerations \ud83d\ude9a",id:"-911-deployment-considerations-",level:2},{value:"\ud83d\udcc8 9.11.1 Real-time Performance Optimization \ud83d\udcc8",id:"-9111-real-time-performance-optimization-",level:3},{value:"\ud83d\udcdd 9.12 Summary \ud83d\udcdd",id:"-912-summary-",level:2},{value:"\u2139\ufe0f Key Takeaways: \u2139\ufe0f",id:"\u2139\ufe0f-key-takeaways-\u2139\ufe0f",level:3},{value:"\ud83e\udd14 Knowledge Check \ud83e\udd14",id:"-knowledge-check-",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"-chapter-9-visual-slam--navigation-nav2-",children:"\ud83d\udcda Chapter 9: Visual SLAM & Navigation (Nav2) \ud83d\udcda"}),"\n",(0,t.jsx)(n.h2,{id:"-learning-objectives-",children:"\ud83c\udfaf Learning Objectives \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement machine learning techniques for robot perception and control"}),"\n",(0,t.jsx)(n.li,{children:"Design deep learning architectures for robotics applications"}),"\n",(0,t.jsx)(n.li,{children:"Apply reinforcement learning algorithms to robotic tasks"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multiple AI systems for cohesive robot behavior"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate and validate AI models in simulation and real-world environments"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"-91-introduction-to-robot-learning-",children:"\ud83d\udc4b 9.1 Introduction to Robot Learning \ud83d\udc4b"}),"\n",(0,t.jsx)(n.p,{children:"Robot learning involves applying machine learning techniques to enable robots to acquire skills, adapt to new situations, and improve performance over time. This encompasses several approaches including supervised learning for perception tasks, reinforcement learning for control problems, and imitation learning for skill acquisition."}),"\n",(0,t.jsx)(n.h3,{id:"-911-types-of-robot-learning-",children:"\ud83c\udfaf 9.1.1 Types of Robot Learning \ud83c\udfaf"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Supervised Learning"}),": Learning from labeled examples (e.g., object recognition from images)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning"}),": Learning through interaction with the environment to maximize rewards"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Imitation Learning"}),": Learning by observing and mimicking human demonstrations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unsupervised Learning"}),": Discovering patterns in data without labeled examples"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Self-supervised Learning"}),": Learning from the structure of data itself"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"-912-challenges-in-robot-learning-",children:"\ud83c\udfaf 9.1.2 Challenges in Robot Learning \ud83c\udfaf"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Safety during learning (especially for physical robots)"}),"\n",(0,t.jsx)(n.li,{children:"Sample efficiency (real-world data collection is expensive)"}),"\n",(0,t.jsx)(n.li,{children:"Sim-to-real transfer (models trained in simulation must work on real robots)"}),"\n",(0,t.jsx)(n.li,{children:"Real-time constraints (many robotic tasks require fast responses)"}),"\n",(0,t.jsx)(n.li,{children:"Multi-modal integration (combining various sensor modalities)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"-92-perception-systems-with-deep-learning-",children:"\ud83c\udfaf 9.2 Perception Systems with Deep Learning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Robots require accurate perception of their environment to make intelligent decisions. Deep learning has revolutionized computer vision and sensor processing for robotics."}),"\n",(0,t.jsx)(n.h3,{id:"\ufe0f-921-convolutional-neural-networks-for-vision-\ufe0f",children:"\ud83d\udc41\ufe0f 9.2.1 Convolutional Neural Networks for Vision \ud83d\udc41\ufe0f"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass RobotVisionCNN(nn.Module):\r\n    def __init__(self, num_classes=10):\r\n        super(RobotVisionCNN, self).__init__()\r\n        # Feature extraction layers\r\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\r\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\r\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\r\n        \r\n        # Pooling layers\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        \r\n        # Fully connected layers\r\n        self.fc1 = nn.Linear(128 * 6 * 6, 512)\r\n        self.fc2 = nn.Linear(512, 256)\r\n        self.fc3 = nn.Linear(256, num_classes)\r\n        \r\n        self.dropout = nn.Dropout(0.5)\r\n\r\n    def forward(self, x):\r\n        # Apply convolutional layers with ReLU and pooling\r\n        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\r\n        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\r\n        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\r\n        \r\n        # Flatten for fully connected layers\r\n        x = x.view(-1, 128 * 6 * 6)  # Adjust based on input size\r\n        x = F.relu(self.fc1(x))\r\n        x = self.dropout(x)\r\n        x = F.relu(self.fc2(x))\r\n        x = self.dropout(x)\r\n        x = self.fc3(x)\r\n        return x\n"})}),"\n",(0,t.jsx)(n.h3,{id:"\u2139\ufe0f-922-semantic-segmentation-for-scene-understanding-\u2139\ufe0f",children:"\u2139\ufe0f 9.2.2 Semantic Segmentation for Scene Understanding \u2139\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"Semantic segmentation helps robots understand the spatial layout of their environment by labeling each pixel in an image with its corresponding object class."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass SemanticSegmentationNet(nn.Module):\r\n    def __init__(self, num_classes):\r\n        super(SemanticSegmentationNet, self).__init__()\r\n        \r\n        # Encoder (feature extraction)\r\n        self.enc_conv1 = nn.Conv2d(3, 64, 3, padding=1)\r\n        self.enc_conv2 = nn.Conv2d(64, 128, 3, padding=1)\r\n        self.enc_conv3 = nn.Conv2d(128, 256, 3, padding=1)\r\n        \r\n        # Decoder (upsampling to full resolution)\r\n        self.dec_conv3 = nn.Conv2d(256, 128, 3, padding=1)\r\n        self.dec_conv2 = nn.Conv2d(128, 64, 3, padding=1)\r\n        self.dec_conv1 = nn.Conv2d(64, num_classes, 3, padding=1)\r\n        \r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\r\n\r\n    def forward(self, x):\r\n        # Encoder\r\n        x1 = F.relu(self.enc_conv1(x))\r\n        x = self.pool(x1)\r\n        \r\n        x2 = F.relu(self.enc_conv2(x))\r\n        x = self.pool(x2)\r\n        \r\n        x = F.relu(self.enc_conv3(x))\r\n        \r\n        # Decoder\r\n        x = self.upsample(x)\r\n        x = F.relu(self.dec_conv3(x + x2))  # Skip connection\r\n        \r\n        x = self.upsample(x)\r\n        x = F.relu(self.dec_conv2(x + x1))  # Skip connection\r\n        \r\n        x = self.upsample(x)\r\n        x = self.dec_conv1(x)\r\n        \r\n        return x\n"})}),"\n",(0,t.jsx)(n.h3,{id:"\u2139\ufe0f-923-depth-estimation-\u2139\ufe0f",children:"\u2139\ufe0f 9.2.3 Depth Estimation \u2139\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"Depth estimation is crucial for navigation and manipulation tasks."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass DepthEstimationNet(nn.Module):\r\n    def __init__(self):\r\n        super(DepthEstimationNet, self).__init__()\r\n        \r\n        # Encoder\r\n        self.encoder = nn.Sequential(\r\n            nn.Conv2d(3, 64, 3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(64, 128, 3, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(128, 256, 3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(2, 2),\r\n            nn.Conv2d(256, 512, 3, padding=1),\r\n            nn.ReLU(),\r\n        )\r\n        \r\n        # Decoder\r\n        self.decoder = nn.Sequential(\r\n            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\r\n            nn.ReLU(),\r\n            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\r\n            nn.ReLU(),\r\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\r\n            nn.ReLU(),\r\n        )\r\n        \r\n        # Output layer for depth map\r\n        self.output = nn.Conv2d(64, 1, 1)\r\n\r\n    def forward(self, x):\r\n        x = self.encoder(x)\r\n        x = self.decoder(x)\r\n        depth_map = self.output(x)\r\n        return depth_map\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-93-robot-control-with-supervised-learning-",children:"\ud83c\udfaf 9.3 Robot Control with Supervised Learning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Supervised learning can be used to learn robot control policies from demonstrations or to map sensor inputs to appropriate motor commands."}),"\n",(0,t.jsx)(n.h3,{id:"-931-sensor-to-action-mapping-",children:"\u26a1 9.3.1 Sensor-to-Action Mapping \u26a1"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\n\r\nclass SensorActionNet(nn.Module):\r\n    def __init__(self, sensor_dim, action_dim):\r\n        super(SensorActionNet, self).__init__()\r\n        \r\n        self.fc1 = nn.Linear(sensor_dim, 256)\r\n        self.fc2 = nn.Linear(256, 256)\r\n        self.fc3 = nn.Linear(256, 128)\r\n        self.fc4 = nn.Linear(128, action_dim)\r\n        \r\n        self.dropout = nn.Dropout(0.2)\r\n\r\n    def forward(self, x):\r\n        x = torch.relu(self.fc1(x))\r\n        x = self.dropout(x)\r\n        x = torch.relu(self.fc2(x))\r\n        x = self.dropout(x)\r\n        x = torch.relu(self.fc3(x))\r\n        x = self.dropout(x)\r\n        x = self.fc4(x)  # Output actions (no activation for continuous control)\r\n        return x\r\n\r\n# \ud83e\udd16 Example usage in a robot controller \ud83e\udd16\r\nclass SupervisedRobotController:\r\n    def __init__(self, model_path):\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        self.model = SensorActionNet(sensor_dim=64, action_dim=12).to(self.device)\r\n        \r\n        # Load pre-trained model\r\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\r\n        self.model.eval()\r\n    \r\n    def get_action(self, sensor_input):\r\n        # Convert sensor input to tensor\r\n        sensor_tensor = torch.FloatTensor(sensor_input).unsqueeze(0).to(self.device)\r\n        \r\n        # Forward pass\r\n        with torch.no_grad():\r\n            action = self.model(sensor_tensor)\r\n        \r\n        # Convert to numpy array for robot control\r\n        return action.cpu().numpy().flatten()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"\ufe0f-932-pid-controllers-enhanced-with-ml-\ufe0f",children:"\ud83c\udf9b\ufe0f 9.3.2 PID Controllers Enhanced with ML \ud83c\udf9b\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"Machine learning can be used to tune PID parameters or enhance traditional controllers:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nfrom scipy import signal\r\n\r\nclass MLPIDController:\r\n    def __init__(self, initial_kp=1.0, initial_ki=0.0, initial_kd=0.0):\r\n        # Basic PID parameters\r\n        self.kp = initial_kp\r\n        self.ki = initial_ki\r\n        self.kd = initial_kd\r\n        \r\n        # Error tracking\r\n        self.previous_error = 0\r\n        self.integral_error = 0\r\n        \r\n        # ML-enhanced parameter adjustment\r\n        self.param_adjustment_model = self._create_param_adjustment_model()\r\n        \r\n    def _create_param_adjustment_model(self):\r\n        # Simple model to adjust PID parameters based on system state\r\n        # In practice, this could be a neural network trained on system data\r\n        return None\r\n\r\n    def update(self, error, dt):\r\n        # Calculate PID terms\r\n        proportional = self.kp * error\r\n        self.integral_error += error * dt\r\n        integral = self.ki * self.integral_error\r\n        derivative = self.kd * (error - self.previous_error) / dt\r\n        \r\n        # Update previous error\r\n        self.previous_error = error\r\n        \r\n        # Calculate control output\r\n        control_output = proportional + integral + derivative\r\n        \r\n        return control_output\r\n\r\n    def adjust_parameters(self, system_state):\r\n        # ML model adjusts the PID parameters based on current system state\r\n        # This is a simplified placeholder implementation\r\n        pass\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-94-deep-learning-architectures-for-robotics-",children:"\ud83c\udfaf 9.4 Deep Learning Architectures for Robotics \ud83c\udfaf"}),"\n",(0,t.jsx)(n.h3,{id:"\u2139\ufe0f-941-recurrent-neural-networks-for-sequential-decision-making-\u2139\ufe0f",children:"\u2139\ufe0f 9.4.1 Recurrent Neural Networks for Sequential Decision Making \u2139\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"RNNs are valuable for tasks that require memory of past states, such as path planning and navigation."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass RobotRNNController(nn.Module):\r\n    def __init__(self, sensor_dim, action_dim, hidden_dim=128, num_layers=2):\r\n        super(RobotRNNController, self).__init__()\r\n        \r\n        self.hidden_dim = hidden_dim\r\n        self.num_layers = num_layers\r\n        \r\n        # RNN layer (using LSTM for better gradient flow)\r\n        self.lstm = nn.LSTM(sensor_dim, hidden_dim, num_layers, batch_first=True)\r\n        \r\n        # Action prediction head\r\n        self.action_head = nn.Linear(hidden_dim, action_dim)\r\n        \r\n        # Initialize hidden state\r\n        self.hidden = None\r\n\r\n    def forward(self, sensor_input, hidden_state=None):\r\n        # sensor_input shape: (batch_size, sequence_length, sensor_dim)\r\n        lstm_out, self.hidden = self.lstm(sensor_input, hidden_state)\r\n        \r\n        # Take the output from the last time step\r\n        last_output = lstm_out[:, -1, :]\r\n        \r\n        # Predict action\r\n        action = self.action_head(last_output)\r\n        \r\n        return action, self.hidden\r\n\r\n    def reset_hidden_state(self):\r\n        self.hidden = None\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-942-attention-mechanisms-for-multi-modal-processing-",children:"\ud83e\udde0 9.4.2 Attention Mechanisms for Multi-Modal Processing \ud83e\udde0"}),"\n",(0,t.jsx)(n.p,{children:"Attention mechanisms can help robots focus on relevant information from multiple sensors."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass MultiModalAttention(nn.Module):\r\n    def __init__(self, modalities_dims, output_dim):\r\n        super(MultiModalAttention, self).__init__()\r\n        \r\n        self.modalities_dims = modalities_dims\r\n        self.num_modalities = len(modalities_dims)\r\n        \r\n        # Linear layers for each modality\r\n        self.modality_transforms = nn.ModuleList([\r\n            nn.Linear(dim, output_dim) for dim in modalities_dims\r\n        ])\r\n        \r\n        # Attention computation\r\n        self.attention_query = nn.Linear(output_dim, output_dim)\r\n        self.attention_key = nn.Linear(output_dim, output_dim)\r\n        self.attention_value = nn.Linear(output_dim, output_dim)\r\n        \r\n        # Output layer\r\n        self.output_layer = nn.Linear(output_dim, output_dim)\r\n\r\n    def forward(self, modalities):\r\n        # modalities: list of tensors, each with shape (batch_size, dim)\r\n        transformed_modalities = []\r\n        \r\n        # Transform each modality to common space\r\n        for i, modality in enumerate(modalities):\r\n            transformed = torch.relu(self.modality_transforms[i](modality))\r\n            transformed_modalities.append(transformed)\r\n        \r\n        # Stack modalities for attention computation\r\n        stacked = torch.stack(transformed_modalities, dim=1)  # (batch_size, num_modalities, output_dim)\r\n        \r\n        # Compute attention\r\n        Q = self.attention_query(stacked)\r\n        K = self.attention_key(stacked)\r\n        V = self.attention_value(stacked)\r\n        \r\n        # Scaled dot-product attention\r\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\r\n        attention_weights = torch.softmax(attention_scores, dim=-1)\r\n        \r\n        # Apply attention to values\r\n        attended = torch.matmul(attention_weights, V)\r\n        \r\n        # Sum across modalities\r\n        output = torch.sum(attended, dim=1)\r\n        output = self.output_layer(output)\r\n        \r\n        return output, attention_weights\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-943-convolutional-lstm-for-spatiotemporal-processing-",children:"\ud83e\udde0 9.4.3 Convolutional LSTM for Spatiotemporal Processing \ud83e\udde0"}),"\n",(0,t.jsx)(n.p,{children:"For tasks that involve both spatial and temporal information, ConvLSTM can be effective:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\n\r\nclass ConvLSTMCell(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\r\n        super(ConvLSTMCell, self).__init__()\r\n        \r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        \r\n        self.kernel_size = kernel_size\r\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\r\n        self.bias = bias\r\n        \r\n        self.conv = nn.Conv2d(\r\n            in_channels=self.input_dim + self.hidden_dim,\r\n            out_channels=4 * self.hidden_dim,\r\n            kernel_size=self.kernel_size,\r\n            padding=self.padding,\r\n            bias=self.bias\r\n        )\r\n\r\n    def forward(self, input_tensor, cur_state):\r\n        h_cur, c_cur = cur_state\r\n        \r\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # Concatenate along channel axis\r\n        \r\n        combined_conv = self.conv(combined)\r\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \r\n        \r\n        i = torch.sigmoid(cc_i)\r\n        f = torch.sigmoid(cc_f)\r\n        o = torch.sigmoid(cc_o)\r\n        g = torch.tanh(cc_g)\r\n        \r\n        c_next = f * c_cur + i * g\r\n        h_next = o * torch.tanh(c_next)\r\n        \r\n        return h_next, c_next\r\n\r\n    def init_hidden(self, batch_size, image_size):\r\n        height, width = image_size\r\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\r\n                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\r\n\r\nclass ConvLSTM(nn.Module):\r\n    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=False, bias=True):\r\n        super(ConvLSTM, self).__init__()\r\n        \r\n        self._check_kernel_size_consistency(kernel_size)\r\n        \r\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\r\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\r\n        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\r\n        \r\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\r\n            raise ValueError('Inconsistent list length.')\r\n        \r\n        self.input_dim = input_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.kernel_size = kernel_size\r\n        self.num_layers = num_layers\r\n        self.batch_first = batch_first\r\n        self.bias = bias\r\n        \r\n        cell_list = []\r\n        for i in range(0, self.num_layers):\r\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\r\n            \r\n            cell_list.append(ConvLSTMCell(\r\n                input_dim=cur_input_dim,\r\n                hidden_dim=self.hidden_dim[i],\r\n                kernel_size=self.kernel_size[i],\r\n                bias=self.bias\r\n            ))\r\n        \r\n        self.cell_list = nn.ModuleList(cell_list)\r\n\r\n    def forward(self, input_tensor, hidden_state=None):\r\n        if not self.batch_first:\r\n            # (t, b, c, h, w) -> (b, t, c, h, w)\r\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\r\n\r\n        b, _, _, h, w = input_tensor.size()\r\n        \r\n        # Implement stateful ConvLSTM\r\n        if hidden_state is not None:\r\n            raise NotImplementedError()\r\n        else:\r\n            # Since the init is done in forward. Can send image size here\r\n            hidden_state = self._init_hidden(batch_size=b, image_size=(h, w))\r\n        \r\n        layer_output_list = []\r\n        cur_layer_input = input_tensor\r\n        \r\n        for layer_idx in range(self.num_layers):\r\n            h, c = hidden_state[layer_idx]\r\n            output_inner = []\r\n            \r\n            for t in range(input_tensor.size(1)):\r\n                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\r\n                                                 cur_state=[h, c])\r\n                output_inner.append(h)\r\n            \r\n            layer_output = torch.stack(output_inner, dim=1)\r\n            cur_layer_input = layer_output\r\n            \r\n            layer_output_list.append(layer_output)\r\n        \r\n        return layer_output_list[-1], hidden_state\r\n\r\n    def _init_hidden(self, batch_size, image_size):\r\n        init_states = []\r\n        for i in range(self.num_layers):\r\n            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\r\n        return init_states\r\n\r\n    @staticmethod\r\n    def _check_kernel_size_consistency(kernel_size):\r\n        if not (isinstance(kernel_size, tuple) or\r\n                (isinstance(kernel_size, list) and all(isinstance(elem, tuple) for elem in kernel_size))):\r\n            raise ValueError('`kernel_size` must be tuple or list of tuples')\r\n\r\n    @staticmethod\r\n    def _extend_for_multilayer(param, num_layers):\r\n        if not isinstance(param, list):\r\n            param = [param] * num_layers\r\n        return param\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-95-reinforcement-learning-for-robotics-",children:"\ud83c\udfaf 9.5 Reinforcement Learning for Robotics \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Reinforcement learning is particularly powerful for robotics as it allows robots to learn complex behaviors through interaction with the environment."}),"\n",(0,t.jsx)(n.h3,{id:"-951-deep-q-network-dqn-for-discrete-actions-",children:"\u26a1 9.5.1 Deep Q-Network (DQN) for Discrete Actions \u26a1"}),"\n",(0,t.jsx)(n.p,{children:"For environments with discrete action spaces, DQN can be effective:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport random\r\nimport numpy as np\r\nfrom collections import deque\r\n\r\nclass DQN(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=64):\r\n        super(DQN, self).__init__()\r\n        \r\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\r\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.fc4 = nn.Linear(hidden_dim, action_dim)\r\n\r\n    def forward(self, x):\r\n        x = torch.relu(self.fc1(x))\r\n        x = torch.relu(self.fc2(x))\r\n        x = torch.relu(self.fc3(x))\r\n        x = self.fc4(x)\r\n        return x\r\n\r\nclass DQNAgent:\r\n    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\r\n        self.state_dim = state_dim\r\n        self.action_dim = action_dim\r\n        self.lr = lr\r\n        self.gamma = gamma\r\n        self.epsilon = epsilon\r\n        self.epsilon_decay = epsilon_decay\r\n        self.epsilon_min = epsilon_min\r\n        \r\n        # Neural networks\r\n        self.q_network = DQN(state_dim, action_dim)\r\n        self.target_network = DQN(state_dim, action_dim)\r\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\r\n        \r\n        # Experience replay buffer\r\n        self.memory = deque(maxlen=10000)\r\n        self.batch_size = 32\r\n        \r\n        # Update target network\r\n        self.update_target_network()\r\n\r\n    def update_target_network(self):\r\n        self.target_network.load_state_dict(self.q_network.state_dict())\r\n\r\n    def remember(self, state, action, reward, next_state, done):\r\n        self.memory.append((state, action, reward, next_state, done))\r\n\r\n    def act(self, state):\r\n        if np.random.random() <= self.epsilon:\r\n            return random.choice(range(self.action_dim))\r\n        \r\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\r\n        q_values = self.q_network(state_tensor)\r\n        return np.argmax(q_values.cpu().data.numpy())\r\n\r\n    def replay(self):\r\n        if len(self.memory) < self.batch_size:\r\n            return\r\n\r\n        batch = random.sample(self.memory, self.batch_size)\r\n        states = torch.FloatTensor([e[0] for e in batch])\r\n        actions = torch.LongTensor([e[1] for e in batch])\r\n        rewards = torch.FloatTensor([e[2] for e in batch])\r\n        next_states = torch.FloatTensor([e[3] for e in batch])\r\n        dones = torch.BoolTensor([e[4] for e in batch])\r\n\r\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\r\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\r\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\r\n\r\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\r\n\r\n        self.optimizer.zero_grad()\r\n        loss.backward()\r\n        self.optimizer.step()\r\n\r\n        if self.epsilon > self.epsilon_min:\r\n            self.epsilon *= self.epsilon_decay\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-952-policy-gradient-methods-ppo-for-continuous-actions-",children:"\u26a1 9.5.2 Policy Gradient Methods (PPO) for Continuous Actions \u26a1"}),"\n",(0,t.jsx)(n.p,{children:"For robotic tasks with continuous action spaces, policy gradient methods like PPO are more appropriate:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\n\r\nclass ActorCritic(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(ActorCritic, self).__init__()\r\n        \r\n        # Shared feature extractor\r\n        self.feature_extractor = nn.Sequential(\r\n            nn.Linear(state_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Actor (policy) network\r\n        self.actor_mean = nn.Linear(hidden_dim, action_dim)\r\n        self.actor_std = nn.Linear(hidden_dim, action_dim)\r\n        \r\n        # Critic (value) network\r\n        self.critic = nn.Linear(hidden_dim, 1)\r\n\r\n    def forward(self, state):\r\n        features = self.feature_extractor(state)\r\n        \r\n        # Actor\r\n        action_mean = torch.tanh(self.actor_mean(features))  # Bound actions to [-1, 1]\r\n        action_log_std = self.actor_std(features)\r\n        action_std = torch.exp(action_log_std)\r\n        \r\n        # Critic\r\n        value = self.critic(features)\r\n        \r\n        return action_mean, action_std, value\r\n\r\nclass PPOAgent:\r\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, clip_epsilon=0.2, epochs=10):\r\n        self.state_dim = state_dim\r\n        self.action_dim = action_dim\r\n        self.lr = lr\r\n        self.gamma = gamma\r\n        self.clip_epsilon = clip_epsilon\r\n        self.epochs = epochs\r\n        \r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        \r\n        # Neural networks\r\n        self.actor_critic = ActorCritic(state_dim, action_dim).to(self.device)\r\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\r\n        \r\n        self.old_actor_critic = ActorCritic(state_dim, action_dim).to(self.device)\r\n        self.old_actor_critic.load_state_dict(self.actor_critic.state_dict())\r\n        \r\n    def select_action(self, state):\r\n        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\r\n        \r\n        with torch.no_grad():\r\n            action_mean, action_std, value = self.old_actor_critic(state)\r\n        \r\n        # Sample action from normal distribution\r\n        dist = torch.distributions.Normal(action_mean, action_std)\r\n        action = dist.sample()\r\n        log_prob = dist.log_prob(action).sum(dim=-1)\r\n        \r\n        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0], value.cpu().numpy()[0]\r\n\r\n    def update(self, states, actions, rewards, dones, log_probs, values):\r\n        states = torch.FloatTensor(states).to(self.device)\r\n        actions = torch.FloatTensor(actions).to(self.device)\r\n        rewards = torch.FloatTensor(rewards).to(self.device)\r\n        dones = torch.BoolTensor(dones).to(self.device)\r\n        old_log_probs = torch.FloatTensor(log_probs).to(self.device)\r\n        \r\n        # Calculate discounted rewards (returns)\r\n        returns = []\r\n        R = 0\r\n        for reward, done in zip(reversed(rewards), reversed(dones)):\r\n            if done:\r\n                R = 0\r\n            R = reward + self.gamma * R\r\n            returns.insert(0, R)\r\n        returns = torch.FloatTensor(returns).to(self.device)\r\n        \r\n        # Normalize advantages\r\n        advantages = returns - values\r\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\r\n        \r\n        # Update policy multiple times\r\n        for _ in range(self.epochs):\r\n            action_means, action_stds, new_values = self.actor_critic(states)\r\n            \r\n            # Calculate new log probabilities\r\n            dist = torch.distributions.Normal(action_means, action_stds)\r\n            new_log_probs = dist.log_prob(actions).sum(dim=-1)\r\n            \r\n            # Calculate ratios\r\n            ratios = torch.exp(new_log_probs - old_log_probs)\r\n            \r\n            # Calculate surrogate objectives\r\n            surr1 = ratios * advantages\r\n            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\r\n            actor_loss = -torch.min(surr1, surr2).mean()\r\n            \r\n            # Calculate critic loss (MSE between predicted values and returns)\r\n            critic_loss = F.mse_loss(new_values.squeeze(), returns)\r\n            \r\n            # Total loss\r\n            total_loss = actor_loss + 0.5 * critic_loss\r\n            \r\n            # Update network\r\n            self.optimizer.zero_grad()\r\n            total_loss.backward()\r\n            self.optimizer.step()\r\n        \r\n        # Update old policy\r\n        self.old_actor_critic.load_state_dict(self.actor_critic.state_dict())\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-953-soft-actor-critic-sac-for-sample-efficient-learning-",children:"\ud83c\udfaf 9.5.3 Soft Actor-Critic (SAC) for Sample Efficient Learning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Soft Actor-Critic is known for its sample efficiency and stability in continuous control tasks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.nn.functional as F\r\nfrom torch.distributions import Normal\r\nimport numpy as np\r\n\r\nclass SACActor(nn.Module):\r\n    def __init__(self, state_dim, action_dim, max_action, hidden_dim=256):\r\n        super(SACActor, self).__init__()\r\n        \r\n        self.l1 = nn.Linear(state_dim, hidden_dim)\r\n        self.l2 = nn.Linear(hidden_dim, hidden_dim)\r\n        \r\n        self.mean_linear = nn.Linear(hidden_dim, action_dim)\r\n        self.log_std_linear = nn.Linear(hidden_dim, action_dim)\r\n        \r\n        self.max_action = max_action\r\n        \r\n    def forward(self, state):\r\n        a = F.relu(self.l1(state))\r\n        a = F.relu(self.l2(a))\r\n        \r\n        mean = self.mean_linear(a)\r\n        log_std = self.log_std_linear(a)\r\n        log_std = torch.clamp(log_std, min=-20, max=2)  # Clamping log_std\r\n        \r\n        return mean, log_std\r\n\r\n    def sample(self, state):\r\n        mean, log_std = self.forward(state)\r\n        std = log_std.exp()\r\n        \r\n        normal = Normal(mean, std)\r\n        x_t = normal.rsample()  # Reparameterization trick\r\n        y_t = torch.tanh(x_t)\r\n        action = y_t * self.max_action\r\n        \r\n        # Compute log probability\r\n        log_prob = normal.log_prob(x_t)\r\n        log_prob -= torch.log(self.max_action * (1 - y_t.pow(2)) + 1e-6)\r\n        log_prob = log_prob.sum(1, keepdim=True)\r\n        \r\n        return action, log_prob\r\n\r\nclass SACCritic(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(SACCritic, self).__init__()\r\n        \r\n        # Q1 network\r\n        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)\r\n        self.l2 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.l3 = nn.Linear(hidden_dim, 1)\r\n        \r\n        # Q2 network\r\n        self.l4 = nn.Linear(state_dim + action_dim, hidden_dim)\r\n        self.l5 = nn.Linear(hidden_dim, hidden_dim)\r\n        self.l6 = nn.Linear(hidden_dim, 1)\r\n\r\n    def forward(self, state, action):\r\n        sa = torch.cat([state, action], 1)\r\n        \r\n        q1 = F.relu(self.l1(sa))\r\n        q1 = F.relu(self.l2(q1))\r\n        q1 = self.l3(q1)\r\n        \r\n        q2 = F.relu(self.l4(sa))\r\n        q2 = F.relu(self.l5(q2))\r\n        q2 = self.l6(q2)\r\n        \r\n        return q1, q2\r\n\r\n    def Q1(self, state, action):\r\n        sa = torch.cat([state, action], 1)\r\n        \r\n        q1 = F.relu(self.l1(sa))\r\n        q1 = F.relu(self.l2(q1))\r\n        q1 = self.l3(q1)\r\n        \r\n        return q1\r\n\r\nclass SACAgent:\r\n    def __init__(self, state_dim, action_dim, max_action, lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2):\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        \r\n        self.actor = SACActor(state_dim, action_dim, max_action).to(self.device)\r\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\r\n        \r\n        self.critic = SACCritic(state_dim, action_dim).to(self.device)\r\n        self.critic_target = SACCritic(state_dim, action_dim).to(self.device)\r\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\r\n        \r\n        # Initialize target networks\r\n        self.critic_target.load_state_dict(self.critic.state_dict())\r\n        \r\n        self.gamma = gamma  # Discount factor\r\n        self.tau = tau  # Soft update parameter\r\n        self.alpha = alpha  # Temperature parameter\r\n        self.action_dim = action_dim\r\n        \r\n    def select_action(self, state):\r\n        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\r\n        action, _ = self.actor.sample(state)\r\n        return action.cpu().data.numpy().flatten()\r\n\r\n    def update(self, replay_buffer, batch_size=256):\r\n        # Sample batch\r\n        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\r\n        \r\n        state = torch.FloatTensor(state).to(self.device)\r\n        action = torch.FloatTensor(action).to(self.device)\r\n        next_state = torch.FloatTensor(next_state).to(self.device)\r\n        reward = torch.FloatTensor(reward).to(self.device).unsqueeze(1)\r\n        not_done = torch.BoolTensor(not_done).to(self.device).unsqueeze(1)\r\n        \r\n        with torch.no_grad():\r\n            next_action, next_log_prob = self.actor.sample(next_state)\r\n            target_q1, target_q2 = self.critic_target(next_state, next_action)\r\n            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_prob\r\n            target_q = reward + not_done * self.gamma * target_q\r\n\r\n        # Critic loss\r\n        current_q1, current_q2 = self.critic(state, action)\r\n        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\r\n        \r\n        # Update critic\r\n        self.critic_optimizer.zero_grad()\r\n        critic_loss.backward()\r\n        self.critic_optimizer.step()\r\n        \r\n        # Compute actor loss\r\n        pi, log_pi = self.actor.sample(state)\r\n        q1, q2 = self.critic(state, pi)\r\n        min_q = torch.min(q1, q2)\r\n        \r\n        actor_loss = ((self.alpha * log_pi) - min_q).mean()\r\n        \r\n        # Update actor\r\n        self.actor_optimizer.zero_grad()\r\n        actor_loss.backward()\r\n        self.actor_optimizer.step()\r\n        \r\n        # Soft update target networks\r\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\r\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"-96-sensor-fusion-with-ai-",children:"\ud83e\udd16 9.6 Sensor Fusion with AI \ud83e\udd16"}),"\n",(0,t.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to provide more accurate, reliable, and robust information than a single sensor could provide."}),"\n",(0,t.jsx)(n.h3,{id:"-961-kalman-filter-implementation-",children:"\ud83d\udd28 9.6.1 Kalman Filter Implementation \ud83d\udd28"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\n\r\nclass KalmanFilter:\r\n    def __init__(self, state_dim, measurement_dim):\r\n        self.state_dim = state_dim\r\n        self.measurement_dim = measurement_dim\r\n        \r\n        # State vector (e.g., [x, y, vx, vy] for 2D position and velocity)\r\n        self.x = np.zeros((state_dim, 1))\r\n        \r\n        # State covariance matrix\r\n        self.P = np.eye(state_dim)\r\n        \r\n        # Process noise covariance\r\n        self.Q = np.eye(state_dim) * 0.1\r\n        \r\n        # Measurement noise covariance\r\n        self.R = np.eye(measurement_dim) * 1.0\r\n        \r\n        # Measurement matrix (maps state to measurement space)\r\n        self.H = np.zeros((measurement_dim, state_dim))\r\n        \r\n        # Control matrix (maps control input to state change)\r\n        self.B = np.zeros((state_dim, 1)) if state_dim > 0 else None\r\n        \r\n        # State transition matrix\r\n        self.F = np.eye(state_dim)\r\n\r\n    def predict(self, u=None):\r\n        """Prediction step - predict state and uncertainty"""\r\n        # State prediction: x = F*x + B*u\r\n        if u is not None:\r\n            self.x = np.dot(self.F, self.x) + np.dot(self.B, u)\r\n        else:\r\n            self.x = np.dot(self.F, self.x)\r\n        \r\n        # Covariance prediction: P = F*P*F^T + Q\r\n        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q\r\n        \r\n        return self.x\r\n\r\n    def update(self, z):\r\n        """Update step - incorporate measurement"""\r\n        # Innovation: y = z - H*x\r\n        y = z - np.dot(self.H, self.x)\r\n        \r\n        # Innovation covariance: S = H*P*H^T + R\r\n        S = np.dot(np.dot(self.H, self.P), self.H.T) + self.R\r\n        \r\n        # Kalman gain: K = P*H^T*S^-1\r\n        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))\r\n        \r\n        # State update: x = x + K*y\r\n        self.x = self.x + np.dot(K, y)\r\n        \r\n        # Covariance update: P = (I - K*H)*P\r\n        I = np.eye(len(self.x))\r\n        self.P = np.dot((I - np.dot(K, self.H)), self.P)\r\n        \r\n        return self.x\r\n\r\n# \ud83e\udd16 Example for robot position tracking \ud83e\udd16\r\nclass RobotKalmanFilter(KalmanFilter):\r\n    def __init__(self):\r\n        # State: [x, y, vx, vy] (position and velocity in 2D)\r\n        super().__init__(state_dim=4, measurement_dim=2)\r\n        \r\n        # For a constant velocity model\r\n        dt = 0.1  # Time step\r\n        self.F = np.array([\r\n            [1, 0, dt, 0],\r\n            [0, 1, 0, dt],\r\n            [0, 0, 1, 0],\r\n            [0, 0, 0, 1]\r\n        ])\r\n        \r\n        # Measurement matrix (we only observe position, not velocity)\r\n        self.H = np.array([\r\n            [1, 0, 0, 0],\r\n            [0, 1, 0, 0]\r\n        ])\r\n        \r\n        # Process noise\r\n        self.Q = np.array([\r\n            [dt**4/4, 0, dt**3/2, 0],\r\n            [0, dt**4/4, 0, dt**3/2],\r\n            [dt**3/2, 0, dt**2, 0],\r\n            [0, dt**3/2, 0, dt**2]\r\n        ]) * 0.1\r\n        \r\n        # Measurement noise\r\n        self.R = np.eye(2) * 0.5  # Measurement uncertainty\r\n\r\n    def predict_position(self):\r\n        return self.x[:2].flatten()  # Return [x, y] position\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-962-particle-filter-implementation-",children:"\ud83d\udd28 9.6.2 Particle Filter Implementation \ud83d\udd28"}),"\n",(0,t.jsx)(n.p,{children:"Particle filters are useful for non-linear, non-Gaussian systems:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\n\r\nclass ParticleFilter:\r\n    def __init__(self, num_particles, state_dim, measurement_dim):\r\n        self.num_particles = num_particles\r\n        self.state_dim = state_dim\r\n        self.measurement_dim = measurement_dim\r\n        \r\n        # Initialize particles randomly\r\n        self.particles = np.random.rand(num_particles, state_dim) * 10  # Random initial states\r\n        self.weights = np.ones(num_particles) / num_particles  # Uniform weights initially\r\n        \r\n    def predict(self, control_input, process_noise_std):\r\n        """Predict step: move particles based on motion model"""\r\n        # Add noise to each particle based on motion model and process noise\r\n        noise = np.random.normal(0, process_noise_std, self.particles.shape)\r\n        self.particles += noise\r\n        \r\n        # Or apply a more complex motion model based on control input\r\n        # self.particles = motion_model(self.particles, control_input)\r\n    \r\n    def update(self, measurement, measurement_noise_std):\r\n        """Update step: compute weights based on measurement likelihood"""\r\n        # Calculate likelihood of each particle given the measurement\r\n        # This is a simplified example for 2D position measurement\r\n        for i in range(self.num_particles):\r\n            # Calculate difference between particle\'s predicted measurement and actual measurement\r\n            predicted_measurement = self.particles[i, :self.measurement_dim]  # Simplified\r\n            diff = measurement - predicted_measurement\r\n            \r\n            # Calculate likelihood using Gaussian probability\r\n            likelihood = np.exp(-0.5 * np.sum((diff)**2) / (measurement_noise_std**2))\r\n            self.weights[i] *= likelihood\r\n        \r\n        # Normalize weights\r\n        self.weights += 1e-300  # Avoid division by zero\r\n        self.weights /= np.sum(self.weights)\r\n    \r\n    def resample(self):\r\n        """Resample particles based on their weights"""\r\n        # Systematic resampling\r\n        indices = np.zeros(self.num_particles, dtype=int)\r\n        cumulative_sum = np.cumsum(self.weights)\r\n        u = np.random.uniform(0, 1/self.num_particles)\r\n        \r\n        i, j = 0, 0\r\n        while i < self.num_particles:\r\n            while u < cumulative_sum[j]:\r\n                indices[i] = j\r\n                u += 1/self.num_particles\r\n                i += 1\r\n            j += 1\r\n        \r\n        # Resample particles\r\n        self.particles = self.particles[indices]\r\n        self.weights.fill(1.0 / self.num_particles)  # Reset weights after resampling\r\n    \r\n    def estimate(self):\r\n        """Get state estimate as weighted average of particles"""\r\n        return np.average(self.particles, weights=self.weights, axis=0)\r\n\r\n# \u2139\ufe0f Example usage \u2139\ufe0f\r\npf = ParticleFilter(num_particles=1000, state_dim=4, measurement_dim=2)  # 4D state, 2D measurement\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-963-neural-network-based-sensor-fusion-",children:"\ud83d\udd17 9.6.3 Neural Network-Based Sensor Fusion \ud83d\udd17"}),"\n",(0,t.jsx)(n.p,{children:"For more advanced fusion approaches using neural networks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass NeuralSensorFusion(nn.Module):\r\n    def __init__(self, sensor_dims, output_dim):\r\n        super(NeuralSensorFusion, self).__init__()\r\n        \r\n        self.sensor_dims = sensor_dims\r\n        self.num_sensors = len(sensor_dims)\r\n        \r\n        # Process each sensor input separately\r\n        self.sensor_processors = nn.ModuleList([\r\n            nn.Sequential(\r\n                nn.Linear(dim, 64),\r\n                nn.ReLU(),\r\n                nn.Linear(64, 32),\r\n                nn.ReLU()\r\n            ) for dim in sensor_dims\r\n        ])\r\n        \r\n        # Fusion layer combining processed sensor data\r\n        fusion_input_dim = 32 * self.num_sensors  # 32 features from each sensor processor\r\n        self.fusion_layer = nn.Sequential(\r\n            nn.Linear(fusion_input_dim, 128),\r\n            nn.ReLU(),\r\n            nn.Linear(128, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, output_dim)\r\n        )\r\n        \r\n        # Attention mechanism to weight sensor importance\r\n        self.attention = nn.Sequential(\r\n            nn.Linear(fusion_input_dim, 64),\r\n            nn.ReLU(),\r\n            nn.Linear(64, self.num_sensors),\r\n            nn.Softmax(dim=1)\r\n        )\r\n\r\n    def forward(self, sensor_inputs):\r\n        # sensor_inputs: list of tensors, each tensor has shape [batch_size, sensor_dim]\r\n        \r\n        processed_sensors = []\r\n        for i, sensor_input in enumerate(sensor_inputs):\r\n            processed = self.sensor_processors[i](sensor_input)\r\n            processed_sensors.append(processed)\r\n        \r\n        # Concatenate all processed sensor data\r\n        concatenated = torch.cat(processed_sensors, dim=1)\r\n        \r\n        # Apply attention to weight sensor importance\r\n        attention_weights = self.attention(concatenated)\r\n        weighted_inputs = []\r\n        \r\n        for i, processed_sensor in enumerate(processed_sensors):\r\n            weight = attention_weights[:, i].unsqueeze(1)  # Shape: [batch, 1]\r\n            weighted_inputs.append(processed_sensor * weight)\r\n        \r\n        # Concatenate weighted inputs\r\n        weighted_concatenated = torch.cat(weighted_inputs, dim=1)\r\n        \r\n        # Final fusion\r\n        output = self.fusion_layer(weighted_concatenated)\r\n        \r\n        return output, attention_weights\r\n\r\n# \u2139\ufe0f Example usage: \u2139\ufe0f\r\n# \ud83d\udce1 sensor_inputs = [lidar_data, camera_features, imu_data] \ud83d\udce1\r\n# \ud83d\udd17 fusion_model = NeuralSensorFusion(sensor_dims=[360, 512, 6], output_dim=128) \ud83d\udd17\r\n# \ud83d\udd17 fused_output, attention_weights = fusion_model(sensor_inputs) \ud83d\udd17\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-97-learning-from-demonstrations-",children:"\ud83c\udfaf 9.7 Learning from Demonstrations \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Imitation learning allows robots to learn complex behaviors by observing human demonstrations."}),"\n",(0,t.jsx)(n.h3,{id:"\u2139\ufe0f-971-behavior-cloning-\u2139\ufe0f",children:"\u2139\ufe0f 9.7.1 Behavior Cloning \u2139\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"Behavior cloning learns a direct mapping from states to actions using supervised learning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\nclass BehaviorCloningNetwork(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(BehaviorCloningNetwork, self).__init__()\r\n        \r\n        self.network = nn.Sequential(\r\n            nn.Linear(state_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, action_dim)\r\n        )\r\n\r\n    def forward(self, state):\r\n        return self.network(state)\r\n\r\nclass BehaviorCloningAgent:\r\n    def __init__(self, state_dim, action_dim, learning_rate=1e-3):\r\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\r\n        \r\n        self.network = BehaviorCloningNetwork(state_dim, action_dim).to(self.device)\r\n        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\r\n        self.criterion = nn.MSELoss()\r\n        \r\n    def train(self, states, actions, epochs=10, batch_size=64):\r\n        # Convert to tensors\r\n        states_tensor = torch.FloatTensor(states).to(self.device)\r\n        actions_tensor = torch.FloatTensor(actions).to(self.device)\r\n        \r\n        # Create dataset and dataloader\r\n        dataset = TensorDataset(states_tensor, actions_tensor)\r\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\r\n        \r\n        self.network.train()\r\n        for epoch in range(epochs):\r\n            total_loss = 0\r\n            for batch_states, batch_actions in dataloader:\r\n                self.optimizer.zero_grad()\r\n                \r\n                # Forward pass\r\n                predicted_actions = self.network(batch_states)\r\n                \r\n                # Compute loss\r\n                loss = self.criterion(predicted_actions, batch_actions)\r\n                \r\n                # Backward pass and optimization\r\n                loss.backward()\r\n                self.optimizer.step()\r\n                \r\n                total_loss += loss.item()\r\n            \r\n            print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")\r\n    \r\n    def predict(self, state):\r\n        self.network.eval()\r\n        with torch.no_grad():\r\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\r\n            action = self.network(state_tensor)\r\n            return action.cpu().numpy().squeeze()\r\n    \r\n    def save_model(self, filepath):\r\n        torch.save(self.network.state_dict(), filepath)\r\n    \r\n    def load_model(self, filepath):\r\n        self.network.load_state_dict(torch.load(filepath, map_location=self.device))\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-972-dagger-dataset-aggregation-",children:"\ud83d\udcca 9.7.2 DAgger (Dataset Aggregation) \ud83d\udcca"}),"\n",(0,t.jsx)(n.p,{children:"DAgger addresses the distribution shift problem in behavior cloning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom collections import deque\r\n\r\nclass DAggerAgent:\r\n    def __init__(self, state_dim, action_dim, expert_policy, learning_rate=1e-3):\r\n        self.state_dim = state_dim\r\n        self.action_dim = action_dim\r\n        self.expert_policy = expert_policy  # Function that takes state and returns expert action\r\n        self.learning_rate = learning_rate\r\n        \r\n        self.network = BehaviorCloningNetwork(state_dim, action_dim)\r\n        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\r\n        self.criterion = nn.MSELoss()\r\n        \r\n        # Buffer to store aggregated dataset\r\n        self.state_buffer = deque(maxlen=10000)\r\n        self.action_buffer = deque(maxlen=10000)\r\n        \r\n    def aggregate_data(self, states, actions_from_expert=True):\r\n        """Add state-action pairs to training dataset"""\r\n        if actions_from_expert:\r\n            # Add state-expert_action pairs\r\n            for state in states:\r\n                expert_action = self.expert_policy(state)\r\n                self.state_buffer.append(state)\r\n                self.action_buffer.append(expert_action)\r\n        else:\r\n            # Add state-expert_action pairs (actions from current policy were corrected by expert)\r\n            for state, action in zip(states, self.predict_batch(states)):\r\n                expert_action = self.expert_policy(state)\r\n                self.state_buffer.append(state)\r\n                self.action_buffer.append(expert_action)\r\n    \r\n    def predict_batch(self, states):\r\n        """Predict actions for a batch of states"""\r\n        self.network.eval()\r\n        with torch.no_grad():\r\n            states_tensor = torch.FloatTensor(states)\r\n            actions = self.network(states_tensor)\r\n            return actions.numpy()\r\n    \r\n    def predict(self, state):\r\n        """Predict action for a single state"""\r\n        self.network.eval()\r\n        with torch.no_grad():\r\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\r\n            action = self.network(state_tensor)\r\n            return action.numpy().squeeze()\r\n    \r\n    def train(self, epochs=10):\r\n        if len(self.state_buffer) < 100:  # Need minimum amount of data\r\n            return\r\n            \r\n        # Convert buffers to tensors\r\n        states_tensor = torch.FloatTensor(list(self.state_buffer))\r\n        actions_tensor = torch.FloatTensor(list(self.action_buffer))\r\n        \r\n        # Create dataset and dataloader\r\n        dataset = TensorDataset(states_tensor, actions_tensor)\r\n        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\r\n        \r\n        self.network.train()\r\n        for epoch in range(epochs):\r\n            total_loss = 0\r\n            for batch_states, batch_actions in dataloader:\r\n                self.optimizer.zero_grad()\r\n                \r\n                # Forward pass\r\n                predicted_actions = self.network(batch_states)\r\n                \r\n                # Compute loss\r\n                loss = self.criterion(predicted_actions, batch_actions)\r\n                \r\n                # Backward pass and optimization\r\n                loss.backward()\r\n                self.optimizer.step()\r\n                \r\n                total_loss += loss.item()\r\n            \r\n            print(f"DAgger Training - Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}")\r\n    \r\n    def get_expert_actions(self, states):\r\n        """Get expert actions for given states"""\r\n        return [self.expert_policy(state) for state in states]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-973-generative-adversarial-imitation-learning-gail-",children:"\ud83c\udfaf 9.7.3 Generative Adversarial Imitation Learning (GAIL) \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"GAIL learns policies by matching the state-action distribution of expert demonstrations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport numpy as np\r\n\r\nclass Discriminator(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\r\n        super(Discriminator, self).__init__()\r\n        \r\n        self.network = nn.Sequential(\r\n            nn.Linear(state_dim + action_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, 1),\r\n            nn.Sigmoid()\r\n        )\r\n\r\n    def forward(self, state, action):\r\n        x = torch.cat([state, action], dim=1)\r\n        return self.network(x)\r\n\r\nclass GAILAgent:\r\n    def __init__(self, policy_network, state_dim, action_dim, learning_rate=1e-4):\r\n        self.state_dim = state_dim\r\n        self.action_dim = action_dim\r\n        \r\n        self.policy_network = policy_network\r\n        self.discriminator = Discriminator(state_dim, action_dim)\r\n        \r\n        self.policy_optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\r\n        self.discriminator_optimizer = optim.Adam(self.discriminator.parameters(), lr=learning_rate)\r\n        \r\n        self.criterion = nn.BCELoss()\r\n        \r\n    def compute_reward(self, state, action):\r\n        """Compute reward as -log(D(s,a)) from discriminator"""\r\n        with torch.no_grad():\r\n            prob_expert = self.discriminator(state, action)\r\n            # Return log(D(s,a)/(1-D(s,a))) which is equivalent to log(D) - log(1-D)\r\n            # But more commonly used is just -log(1-D(s,a)) which encourages D to go to 1\r\n            reward = -torch.log(1 - prob_expert + 1e-8)  # Add small epsilon to avoid log(0)\r\n            return reward.squeeze()\r\n    \r\n    def discriminator_loss(self, expert_states, expert_actions, policy_states, policy_actions):\r\n        # Expert data should be classified as expert (1)\r\n        expert_labels = torch.ones(expert_states.size(0), 1)\r\n        expert_loss = self.criterion(\r\n            self.discriminator(expert_states, expert_actions), \r\n            expert_labels\r\n        )\r\n        \r\n        # Policy data should be classified as not expert (0)\r\n        policy_labels = torch.zeros(policy_states.size(0), 1)\r\n        policy_loss = self.criterion(\r\n            self.discriminator(policy_states, policy_actions), \r\n            policy_labels\r\n        )\r\n        \r\n        return expert_loss + policy_loss\r\n    \r\n    def update_discriminator(self, expert_states, expert_actions, policy_states, policy_actions):\r\n        """Update discriminator to better distinguish expert vs policy data"""\r\n        self.discriminator_optimizer.zero_grad()\r\n        \r\n        loss = self.discriminator_loss(\r\n            expert_states, expert_actions, \r\n            policy_states, policy_actions\r\n        )\r\n        \r\n        loss.backward()\r\n        self.discriminator_optimizer.step()\r\n        \r\n        return loss.item()\r\n    \r\n    def update_policy(self, states, actions, rewards):\r\n        """Update policy to maximize discriminator confusion (minimize discriminator output)"""\r\n        self.policy_optimizer.zero_grad()\r\n        \r\n        # Get new actions from updated policy\r\n        new_actions = self.policy_network(states)\r\n        \r\n        # Discriminator should output low value for policy actions\r\n        disc_output = self.discriminator(states, new_actions)\r\n        policy_loss = -torch.log(disc_output + 1e-8).mean()  # Maximize log(1-D) equivalent\r\n        \r\n        policy_loss.backward()\r\n        self.policy_optimizer.step()\r\n        \r\n        return policy_loss.item()\r\n\r\n# \u2139\ufe0f Example usage: \u2139\ufe0f\r\n# \u26a1 policy_network = SomePolicyNetwork(state_dim, action_dim) \u26a1\r\n# \ud83e\udd16 gail_agent = GAILAgent(policy_network, state_dim, action_dim) \ud83e\udd16\n'})}),"\n",(0,t.jsx)(n.h2,{id:"-98-navigation-and-path-planning-with-ai-",children:"\ud83e\udd16 9.8 Navigation and Path Planning with AI \ud83e\udd16"}),"\n",(0,t.jsx)(n.p,{children:"Robot navigation requires planning efficient, collision-free paths through environments."}),"\n",(0,t.jsx)(n.h3,{id:"\u2139\ufe0f-981-a-algorithm-with-neural-heuristics-\u2139\ufe0f",children:"\u2139\ufe0f 9.8.1 A* Algorithm with Neural Heuristics \u2139\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"Traditional A* can be enhanced with learned heuristics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import heapq\r\nimport numpy as np\r\n\r\nclass NeuralHeuristicAStar:\r\n    def __init__(self, grid_map, neural_heuristic_model):\r\n        self.grid_map = grid_map  # 2D grid where 0=free, 1=obstacle\r\n        self.neural_heuristic = neural_heuristic_model  # Pre-trained neural network\r\n        self.rows, self.cols = grid_map.shape\r\n        \r\n    def heuristic(self, pos, goal):\r\n        """Neural network-based heuristic function"""\r\n        # Convert position and goal to feature vector for the neural network\r\n        features = np.array([pos[0], pos[1], goal[0], goal[1]]).astype(np.float32)\r\n        features = torch.FloatTensor(features).unsqueeze(0)\r\n        \r\n        with torch.no_grad():\r\n            heuristic_value = self.neural_heuristic(features)\r\n        \r\n        return heuristic_value.item()\r\n    \r\n    def get_neighbors(self, pos):\r\n        """Get valid neighbors for a position"""\r\n        neighbors = []\r\n        for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0), (1, 1), (1, -1), (-1, 1), (-1, -1)]:\r\n            nr, nc = pos[0] + dr, pos[1] + dc\r\n            if 0 <= nr < self.rows and 0 <= nc < self.cols and self.grid_map[nr][nc] == 0:\r\n                # Add cost based on grid value (for weighted grids)\r\n                move_cost = 1.0 if abs(dr) + abs(dc) == 1 else 1.414  # Diagonal moves cost more\r\n                neighbors.append(((nr, nc), move_cost))\r\n        return neighbors\r\n    \r\n    def plan_path(self, start, goal):\r\n        """Plan a path from start to goal using A* with neural heuristic"""\r\n        # Priority queue: (f_score, g_score, position)\r\n        open_set = [(0, 0, start)]\r\n        \r\n        # Costs: g_score (actual cost from start) and f_score (g + heuristic)\r\n        g_score = {start: 0}\r\n        f_score = {start: self.heuristic(start, goal)}\r\n        \r\n        # Track path parents\r\n        came_from = {}\r\n        \r\n        while open_set:\r\n            current_f, current_g, current = heapq.heappop(open_set)\r\n            \r\n            if current == goal:\r\n                # Reconstruct path\r\n                path = []\r\n                while current in came_from:\r\n                    path.append(current)\r\n                    current = came_from[current]\r\n                path.append(start)\r\n                return path[::-1]  # Reverse to get path from start to goal\r\n            \r\n            for neighbor, move_cost in self.get_neighbors(current):\r\n                tentative_g = current_g + move_cost\r\n                \r\n                if neighbor not in g_score or tentative_g < g_score[neighbor]:\r\n                    came_from[neighbor] = current\r\n                    g_score[neighbor] = tentative_g\r\n                    f_score[neighbor] = tentative_g + self.heuristic(neighbor, goal)\r\n                    heapq.heappush(open_set, (f_score[neighbor], tentative_g, neighbor))\r\n        \r\n        return None  # No path found\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-982-learning-based-path-planning-",children:"\ud83c\udfaf 9.8.2 Learning-based Path Planning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Using neural networks to directly learn navigation policies:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass NavigationPolicy(nn.Module):\r\n    def __init__(self, map_channels=1, goal_dim=2, hidden_dim=128, action_dim=4):\r\n        super(NavigationPolicy, self).__init__()\r\n        \r\n        # CNN to process map information\r\n        self.map_cnn = nn.Sequential(\r\n            nn.Conv2d(map_channels, 32, kernel_size=3, stride=1, padding=1),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\r\n            nn.ReLU(),\r\n            nn.AdaptiveAvgPool2d((8, 8))  # Reduce spatial dimensions\r\n        )\r\n        \r\n        # Process goal information\r\n        self.goal_fc = nn.Sequential(\r\n            nn.Linear(goal_dim, 64),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Combine map and goal features\r\n        cnn_output_size = 64 * 8 * 8  # After CNN and pooling\r\n        combined_input_size = cnn_output_size + 64  # + goal features\r\n        \r\n        self.combined_layers = nn.Sequential(\r\n            nn.Linear(combined_input_size, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU()\r\n        )\r\n        \r\n        # Output layers for action and value\r\n        self.action_head = nn.Linear(hidden_dim, action_dim)\r\n        self.value_head = nn.Linear(hidden_dim, 1)\r\n\r\n    def forward(self, map_tensor, goal_pos):\r\n        # Process map with CNN\r\n        map_features = self.map_cnn(map_tensor)\r\n        map_features = map_features.view(map_features.size(0), -1)  # Flatten\r\n        \r\n        # Process goal\r\n        goal_features = self.goal_fc(goal_pos)\r\n        \r\n        # Combine features\r\n        combined = torch.cat([map_features, goal_features], dim=1)\r\n        \r\n        # Process combined features\r\n        hidden = self.combined_layers(combined)\r\n        \r\n        # Output action logits and value\r\n        action_logits = self.action_head(hidden)\r\n        value = self.value_head(hidden)\r\n        \r\n        return action_logits, value\r\n\r\nclass LearningBasedNavigator:\r\n    def __init__(self, policy_network, device='cpu'):\r\n        self.policy_network = policy_network\r\n        self.device = device\r\n        \r\n    def get_action(self, map_tensor, goal_pos, available_actions=None):\r\n        self.policy_network.eval()\r\n        \r\n        with torch.no_grad():\r\n            map_tensor = map_tensor.to(self.device).unsqueeze(0)  # Add batch dimension\r\n            goal_pos = goal_pos.to(self.device).unsqueeze(0)  # Add batch dimension\r\n            \r\n            action_logits, _ = self.policy_network(map_tensor, goal_pos)\r\n            \r\n            if available_actions is not None:\r\n                # Mask out unavailable actions\r\n                masked_logits = torch.full_like(action_logits, float('-inf'))\r\n                masked_logits[0, available_actions] = action_logits[0, available_actions]\r\n                action_probs = F.softmax(masked_logits, dim=-1)\r\n            else:\r\n                action_probs = F.softmax(action_logits, dim=-1)\r\n            \r\n            # Sample action from distribution or take argmax\r\n            action = torch.multinomial(action_probs, 1).item()\r\n        \r\n        return action\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-983-rrt-with-learning-enhancement-",children:"\ud83c\udfaf 9.8.3 RRT* with Learning Enhancement \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Rapidly-exploring Random Trees (RRT*) can be enhanced with learning to guide exploration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport random\r\nfrom scipy.spatial import KDTree\r\n\r\nclass LearningEnhancedRRTStar:\r\n    def __init__(self, bounds, start, goal, obstacle_list, goal_bias=0.05):\r\n        self.bounds = bounds  # [(min_x, max_x), (min_y, max_y), ...]\r\n        self.start = start\r\n        self.goal = goal\r\n        self.obstacles = obstacle_list\r\n        self.goal_bias = goal_bias  # Probability of sampling goal\r\n        \r\n        # For RRT*: radius for choosing parents and rewiring\r\n        self.r = 1.5\r\n        \r\n        # Graph storage\r\n        self.vertices = [start]\r\n        self.edges = {}  # vertex -> list of (neighbor, cost)\r\n        self.costs = {tuple(start): 0.0}  # vertex -> cost from start\r\n        self.parents = {tuple(start): None}\r\n        \r\n        # Learning component: bias sampling toward promising areas\r\n        self.visit_counts = {tuple(start): 0}\r\n        \r\n    def is_collision_free(self, point1, point2):\r\n        """Check if path between two points is collision-free"""\r\n        # Simple implementation; in practice, you\'d want more sophisticated collision checking\r\n        num_samples = 10\r\n        for i in range(num_samples + 1):\r\n            t = i / num_samples\r\n            point = point1 * (1 - t) + point2 * t\r\n            \r\n            for obs in self.obstacles:\r\n                if np.linalg.norm(point - obs[:2]) <= obs[2]:  # Assuming circular obstacles\r\n                    return False\r\n        return True\r\n    \r\n    def sample_free(self):\r\n        """Sample a free configuration"""\r\n        # With some probability, sample the goal\r\n        if random.random() < self.goal_bias:\r\n            return self.goal\r\n        \r\n        # Otherwise, sample based on learned preferences\r\n        # For now, uniform random sampling with bounds\r\n        point = np.array([random.uniform(bound[0], bound[1]) for bound in self.bounds])\r\n        \r\n        # Check if point is in free space\r\n        for obs in self.obstacles:\r\n            if np.linalg.norm(point - obs[:2]) <= obs[2]:\r\n                # If in obstacle, try again\r\n                return self.sample_free()\r\n        \r\n        return point\r\n    \r\n    def nearest_vertex(self, point):\r\n        """Find nearest vertex in tree to given point"""\r\n        points = np.array(self.vertices)\r\n        tree = KDTree(points)\r\n        dist, idx = tree.query(point)\r\n        return self.vertices[idx]\r\n    \r\n    def extend_tree(self, new_point):\r\n        """Extend the tree toward a new point"""\r\n        nearest = self.nearest_vertex(new_point)\r\n        \r\n        # Try to connect to all vertices within radius that are collision-free\r\n        valid_parents = []\r\n        for vertex in self.vertices:\r\n            dist = np.linalg.norm(np.array(vertex) - np.array(new_point))\r\n            if dist <= self.r and self.is_collision_free(vertex, new_point):\r\n                total_cost = self.costs[tuple(vertex)] + dist\r\n                valid_parents.append((vertex, total_cost))\r\n        \r\n        if not valid_parents:\r\n            return False  # Cannot connect\r\n        \r\n        # Choose parent with minimum cost\r\n        parent, min_cost = min(valid_parents, key=lambda x: x[1])\r\n        \r\n        # Add new vertex and edge\r\n        self.vertices.append(new_point)\r\n        parent_key = tuple(parent)\r\n        new_point_key = tuple(new_point)\r\n        \r\n        if parent_key not in self.edges:\r\n            self.edges[parent_key] = []\r\n        self.edges[parent_key].append((new_point, np.linalg.norm(np.array(parent) - np.array(new_point))))\r\n        \r\n        self.costs[new_point_key] = min_cost\r\n        self.parents[new_point_key] = parent_key\r\n        \r\n        # Rewire: Check if we can improve costs of nearby vertices through new point\r\n        for vertex in self.vertices[:-1]:  # Exclude the new point\r\n            vertex_key = tuple(vertex)\r\n            dist_to_new = np.linalg.norm(np.array(vertex) - np.array(new_point))\r\n            if dist_to_new <= self.r and self.is_collision_free(new_point, vertex):\r\n                new_cost = min_cost + dist_to_new\r\n                if new_cost < self.costs[vertex_key]:\r\n                    # Update parent of vertex\r\n                    old_parent = self.parents[vertex_key]\r\n                    self.parents[vertex_key] = new_point_key\r\n                    \r\n                    # Update edges: remove old edge, add new one\r\n                    if old_parent in self.edges:\r\n                        self.edges[old_parent] = [(v, c) for v, c in self.edges[old_parent] if not np.array_equal(v, vertex)]\r\n                    \r\n                    if new_point_key not in self.edges:\r\n                        self.edges[new_point_key] = []\r\n                    self.edges[new_point_key].append((vertex, dist_to_new))\r\n                    \r\n                    # Update cost\r\n                    self.costs[vertex_key] = new_cost\r\n        \r\n        return True\r\n    \r\n    def plan(self, max_iterations=1000):\r\n        """Plan a path using RRT*"""\r\n        for i in range(max_iterations):\r\n            new_point = self.sample_free()\r\n            self.extend_tree(new_point)\r\n            \r\n            # Check if we\'ve reached the goal region\r\n            for vertex in self.vertices:\r\n                if np.linalg.norm(np.array(vertex) - np.array(self.goal)) < 0.5:  # Goal region threshold\r\n                    return self.reconstruct_path(tuple(self.goal))\r\n        \r\n        return None  # No path found\r\n    \r\n    def reconstruct_path(self, goal_point_key):\r\n        """Reconstruct path from goal to start"""\r\n        path = []\r\n        current = goal_point_key\r\n        \r\n        while current is not None:\r\n            path.append(np.array(current))\r\n            current = self.parents[current]\r\n        \r\n        return path[::-1]  # Reverse to get path from start to goal\n'})}),"\n",(0,t.jsx)(n.h2,{id:"-99-human-robot-interaction-learning-",children:"\ud83c\udfaf 9.9 Human-Robot Interaction Learning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.h3,{id:"-991-learning-from-human-feedback-lfhf-",children:"\ud83c\udfaf 9.9.1 Learning from Human Feedback (LfHF) \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Learning from human feedback allows robots to align their behavior with human preferences:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport numpy as np\r\n\r\nclass PreferenceModel(nn.Module):\r\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\r\n        super(PreferenceModel, self).__init__()\r\n        \r\n        self.state_action_dim = state_dim + action_dim\r\n        self.network = nn.Sequential(\r\n            nn.Linear(self.state_action_dim * 2, hidden_dim),  # Two state-action pairs\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, hidden_dim),\r\n            nn.ReLU(),\r\n            nn.Linear(hidden_dim, 1),  # Output preference probability\r\n            nn.Sigmoid()\r\n        )\r\n    \r\n    def forward(self, state1, action1, state2, action2):\r\n        # Concatenate both state-action pairs\r\n        sa1 = torch.cat([state1, action1], dim=-1)\r\n        sa2 = torch.cat([state2, action2], dim=-1)\r\n        combined = torch.cat([sa1, sa2], dim=-1)\r\n        \r\n        return self.network(combined)\r\n\r\nclass LearningFromHumanFeedback:\r\n    def __init__(self, state_dim, action_dim, policy_network, learning_rate=1e-4):\r\n        self.state_dim = state_dim\r\n        self.action_dim = action_dim\r\n        \r\n        self.preference_model = PreferenceModel(state_dim, action_dim)\r\n        self.policy_network = policy_network  # The policy being trained\r\n        \r\n        self.optimizer = optim.Adam(self.preference_model.parameters(), lr=learning_rate)\r\n        self.criterion = nn.BCELoss()\r\n        \r\n    def get_preference_prediction(self, state1, action1, state2, action2):\r\n        """Get the model\'s prediction of which state-action pair is preferred"""\r\n        with torch.no_grad():\r\n            prob = self.preference_model(state1, action1, state2, action2)\r\n            return prob.item()\r\n    \r\n    def update_preference_model(self, preferred_pairs, unpreferred_pairs):\r\n        """Update the preference model based on human feedback"""\r\n        # preferred_pairs and unpreferred_pairs are lists of (state, action) tuples\r\n        # where preferred_pairs[0] is preferred to unpreferred_pairs[0], etc.\r\n        \r\n        # Create batch tensors\r\n        pref_states = torch.stack([pair[0] for pair in preferred_pairs])\r\n        pref_actions = torch.stack([pair[1] for pair in preferred_pairs])\r\n        unprefer_states = torch.stack([pair[0] for pair in unpreferred_pairs])\r\n        unprefer_actions = torch.stack([pair[1] for pair in unpreferred_pairs])\r\n        \r\n        self.optimizer.zero_grad()\r\n        \r\n        # Predict preference: probability that first option is preferred\r\n        prob_first_preferred = self.preference_model(\r\n            pref_states, pref_actions, \r\n            unprefer_states, unprefer_actions\r\n        )\r\n        \r\n        # Since first is preferred, target should be 1\r\n        target = torch.ones_like(prob_first_preferred)\r\n        \r\n        loss = self.criterion(prob_first_preferred, target)\r\n        loss.backward()\r\n        self.optimizer.step()\r\n        \r\n        return loss.item()\r\n    \r\n    def get_action_rankings(self, state, action_candidates):\r\n        """Rank a set of action candidates for a given state"""\r\n        rankings = []\r\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\r\n        \r\n        for action in action_candidates:\r\n            action_tensor = torch.FloatTensor(action).unsqueeze(0)\r\n            \r\n            # Compare each action to a baseline (could be random or policy action)\r\n            baseline_action = torch.randn_like(action_tensor)  # Random baseline\r\n            \r\n            preference = self.get_preference_prediction(\r\n                state_tensor, action_tensor,\r\n                state_tensor, baseline_action\r\n            )\r\n            \r\n            rankings.append((action, preference))\r\n        \r\n        # Sort by preference (descending)\r\n        rankings.sort(key=lambda x: x[1], reverse=True)\r\n        return rankings\n'})}),"\n",(0,t.jsx)(n.h3,{id:"-992-interactive-robot-learning-",children:"\ud83c\udfaf 9.9.2 Interactive Robot Learning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Interactive learning allows robots to ask questions to clarify human intentions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class InteractiveLearningAgent:\r\n    def __init__(self, robot_policy, question_generation_model):\r\n        self.policy = robot_policy\r\n        self.question_model = question_generation_model\r\n        self.uncertainty_threshold = 0.2  # Threshold for asking questions\r\n        \r\n        # Track learned preferences/parameters\r\n        self.human_preferences = {}\r\n        self.task_parameters = {}\r\n        \r\n    def should_ask_question(self, state, action_distribution):\r\n        """Determine if the robot should ask a question based on uncertainty"""\r\n        # Calculate entropy of action distribution as uncertainty measure\r\n        entropy = -sum(p * np.log(p + 1e-10) for p in action_distribution)\r\n        max_entropy = np.log(len(action_distribution))  # Max possible entropy\r\n        \r\n        normalized_entropy = entropy / max_entropy\r\n        \r\n        return normalized_entropy > self.uncertainty_threshold\r\n    \r\n    def generate_question(self, state, current_task):\r\n        """Generate an appropriate question based on state and task"""\r\n        # This is a simplified example; in practice, this could be a learned model\r\n        # that generates questions based on the state and current task uncertainty\r\n        \r\n        # Based on state and task, determine what information is most needed\r\n        if current_task == "navigation":\r\n            # Generate navigation-specific questions\r\n            return {\r\n                "type": "preference_query",\r\n                "question": "Which path would you prefer: the shortest or the safest?",\r\n                "options": ["shortest", "safest"]\r\n            }\r\n        elif current_task == "manipulation":\r\n            # Generate manipulation-specific questions\r\n            return {\r\n                "type": "parameter_query", \r\n                "question": "How gently should I grasp the object?",\r\n                "range": [0.1, 1.0]  # Force range from gentle to firm\r\n            }\r\n        else:\r\n            # Default question\r\n            return {\r\n                "type": "clarification",\r\n                "question": f"What would you like me to do now?",\r\n                "options": ["continue", "stop", "repeat"]\r\n            }\r\n    \r\n    def incorporate_feedback(self, question, answer):\r\n        """Incorporate human feedback into the model"""\r\n        if question["type"] == "preference_query":\r\n            self.human_preferences[question["question"]] = answer\r\n        elif question["type"] == "parameter_query":\r\n            self.task_parameters[question["question"]] = answer\r\n        elif question["type"] == "clarification":\r\n            self.task_parameters["next_action"] = answer\r\n    \r\n    def get_adapted_action(self, state, current_task):\r\n        """Get action adapted based on learned preferences"""\r\n        # First, check if we should ask a question\r\n        action_probs = self.policy.get_action_probabilities(state)\r\n        \r\n        if self.should_ask_question(state, action_probs):\r\n            question = self.generate_question(state, current_task)\r\n            # In a real implementation, you\'d present this to the human\r\n            # For now, we\'ll simulate getting an answer\r\n            simulated_answer = self.simulate_human_answer(question)\r\n            self.incorporate_feedback(question, simulated_answer)\r\n            \r\n            # Recompute action based on incorporated feedback\r\n            adapted_action_probs = self.policy.get_adapted_action_probabilities(\r\n                state, self.human_preferences, self.task_parameters\r\n            )\r\n            return np.random.choice(len(adapted_action_probs), p=adapted_action_probs)\r\n        else:\r\n            # Just take the most probable action\r\n            return np.argmax(action_probs)\r\n    \r\n    def simulate_human_answer(self, question):\r\n        """Simulate human response (in real implementation, this would be actual human input)"""\r\n        if question["type"] == "preference_query":\r\n            return question["options"][0]  # Simulate always preferring the first option\r\n        elif question["type"] == "parameter_query":\r\n            return question["range"][1] * 0.7  # Simulate preferring 70% of max value\r\n        else:  # clarification\r\n            return question["options"][0]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"-910-model-evaluation-and-validation-",children:"\ud83d\udcca 9.10 Model Evaluation and Validation \ud83d\udcca"}),"\n",(0,t.jsx)(n.h3,{id:"-9101-metrics-for-robot-learning-",children:"\ud83c\udfaf 9.10.1 Metrics for Robot Learning \ud83c\udfaf"}),"\n",(0,t.jsx)(n.p,{children:"Evaluating robot learning systems requires specialized metrics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nclass RobotLearningEvaluator:\r\n    def __init__(self):\r\n        self.episode_rewards = []\r\n        self.episode_lengths = []\r\n        self.success_rates = []\r\n        self.collision_rates = []\r\n        \r\n    def add_episode_data(self, reward, length, success, collision):\r\n        \"\"\"Add data from a completed episode\"\"\"\r\n        self.episode_rewards.append(reward)\r\n        self.episode_lengths.append(length)\r\n        self.success_rates.append(success)\r\n        self.collision_rates.append(collision)\r\n    \r\n    def compute_episode_metrics(self):\r\n        \"\"\"Compute metrics based on collected episode data\"\"\"\r\n        if not self.episode_rewards:\r\n            return {}\r\n        \r\n        metrics = {\r\n            'average_reward': np.mean(self.episode_rewards),\r\n            'std_reward': np.std(self.episode_rewards),\r\n            'average_length': np.mean(self.episode_lengths),\r\n            'success_rate': np.mean(self.success_rates),\r\n            'collision_rate': np.mean(self.collision_rates),\r\n            'total_episodes': len(self.episode_rewards)\r\n        }\r\n        \r\n        return metrics\r\n    \r\n    def compute_safety_metrics(self):\r\n        \"\"\"Compute safety-related metrics\"\"\"\r\n        if not self.collision_rates:\r\n            return {}\r\n        \r\n        return {\r\n            'collision_rate': np.mean(self.collision_rates),\r\n            'min_distance_to_obstacles': np.min(self.episode_rewards) if self.episode_rewards else float('inf'),\r\n            'average_velocity_magnitude': np.mean([np.linalg.norm(r) for r in self.episode_rewards]) if self.episode_rewards else 0\r\n        }\r\n    \r\n    def compute_efficiency_metrics(self):\r\n        \"\"\"Compute efficiency metrics\"\"\"\r\n        if not self.episode_lengths or not self.success_rates:\r\n            return {}\r\n        \r\n        successful_episodes = [l for l, s in zip(self.episode_lengths, self.success_rates) if s]\r\n        \r\n        return {\r\n            'average_completion_time': np.mean(successful_episodes) if successful_episodes else float('inf'),\r\n            'task_completion_rate': np.sum(self.success_rates) / len(self.success_rates) if self.success_rates else 0,\r\n            'average_path_efficiency': self.compute_path_efficiency()  # Placeholder implementation\r\n        }\r\n    \r\n    def compute_path_efficiency(self):\r\n        \"\"\"Compute how direct paths are compared to optimal\"\"\"\r\n        # Placeholder: In practice, you'd compare actual path length to optimal path length\r\n        return 0.85  # Example efficiency value\r\n    \r\n    def plot_learning_curves(self):\r\n        \"\"\"Plot learning curves\"\"\"\r\n        if not self.episode_rewards:\r\n            return\r\n            \r\n        # Create subplots for different metrics\r\n        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\r\n        \r\n        # Plot cumulative rewards\r\n        cumulative_rewards = np.cumsum(self.episode_rewards)\r\n        axes[0, 0].plot(cumulative_rewards)\r\n        axes[0, 0].set_title('Cumulative Rewards Over Time')\r\n        axes[0, 0].set_xlabel('Episode')\r\n        axes[0, 0].set_ylabel('Cumulative Reward')\r\n        \r\n        # Plot rolling average of rewards\r\n        if len(self.episode_rewards) > 10:\r\n            rolling_avg = np.convolve(self.episode_rewards, np.ones(10)/10, mode='valid')\r\n            axes[0, 1].plot(rolling_avg)\r\n            axes[0, 1].set_title('Rolling Average of Rewards (window=10)')\r\n            axes[0, 1].set_xlabel('Episode')\r\n            axes[0, 1].set_ylabel('Average Reward')\r\n        \r\n        # Plot success rate over time\r\n        if len(self.success_rates) > 10:\r\n            rolling_success = np.convolve(self.success_rates, np.ones(10)/10, mode='valid')\r\n            axes[1, 0].plot(rolling_success)\r\n            axes[1, 0].set_title('Rolling Average Success Rate (window=10)')\r\n            axes[1, 0].set_xlabel('Episode')\r\n            axes[1, 0].set_ylabel('Success Rate')\r\n            axes[1, 0].set_ylim(0, 1)\r\n        \r\n        # Plot collision rate over time\r\n        if len(self.collision_rates) > 10:\r\n            rolling_collision = np.convolve(self.collision_rates, np.ones(10)/10, mode='valid')\r\n            axes[1, 1].plot(rolling_collision, color='red')\r\n            axes[1, 1].set_title('Rolling Average Collision Rate (window=10)')\r\n            axes[1, 1].set_xlabel('Episode')\r\n            axes[1, 1].set_ylabel('Collision Rate')\r\n            axes[1, 1].set_ylim(0, 1)\r\n        \r\n        plt.tight_layout()\r\n        plt.show()\r\n        \r\n        return fig\n"})}),"\n",(0,t.jsx)(n.h3,{id:"-9102-simulation-to-real-transfer-validation-",children:"\ud83c\udfae 9.10.2 Simulation-to-Real Transfer Validation \ud83c\udfae"}),"\n",(0,t.jsx)(n.p,{children:"Validating that models trained in simulation work in the real world:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class SimToRealValidator:\r\n    def __init__(self, sim_model, real_robot_interface, domain_randomization_params=None):\r\n        self.sim_model = sim_model\r\n        self.real_robot = real_robot_interface\r\n        self.domain_params = domain_randomization_params or {}\r\n        \r\n        # Performance tracking\r\n        self.sim_performance = {}\r\n        self.real_performance = {}\r\n        \r\n    def validate_perception(self, test_data_sim, test_data_real):\r\n        \"\"\"Validate perception models between sim and real\"\"\"\r\n        # Run perception on simulation data\r\n        sim_predictions = self.sim_model.predict(test_data_sim)\r\n        \r\n        # Run on real data (requires real perception model)\r\n        real_predictions = self.real_robot.run_perception(test_data_real)\r\n        \r\n        # Compare results\r\n        perception_accuracy = self.compute_similarity(sim_predictions, real_predictions)\r\n        \r\n        return {\r\n            'sim_accuracy': sim_predictions.accuracy if hasattr(sim_predictions, 'accuracy') else None,\r\n            'real_accuracy': real_predictions.accuracy if hasattr(real_predictions, 'accuracy') else None,\r\n            'sim_real_similarity': perception_accuracy\r\n        }\r\n    \r\n    def validate_control_policy(self, num_trials=20):\r\n        \"\"\"Validate control policy between sim and real\"\"\"\r\n        sim_rewards = []\r\n        real_rewards = []\r\n        \r\n        # Test in simulation\r\n        for _ in range(num_trials):\r\n            episode_reward = self.evaluate_policy_in_sim(self.sim_model)\r\n            sim_rewards.append(episode_reward)\r\n        \r\n        # Test on real robot\r\n        for _ in range(num_trials):\r\n            episode_reward = self.evaluate_policy_on_real(self.sim_model)\r\n            real_rewards.append(episode_reward)\r\n        \r\n        return {\r\n            'sim_mean_reward': np.mean(sim_rewards),\r\n            'sim_std_reward': np.std(sim_rewards),\r\n            'real_mean_reward': np.mean(real_rewards), \r\n            'real_std_reward': np.std(real_rewards),\r\n            'sim_real_correlation': np.corrcoef(sim_rewards, real_rewards)[0, 1] if len(sim_rewards) > 1 else 0\r\n        }\r\n    \r\n    def compute_similarity(self, sim_output, real_output):\r\n        \"\"\"Compute similarity between sim and real outputs\"\"\"\r\n        # This is a simplified approach; in practice, you'd use domain-specific similarity measures\r\n        if hasattr(sim_output, 'features') and hasattr(real_output, 'features'):\r\n            # Compare feature vectors using cosine similarity\r\n            sim_features = np.array(sim_output.features)\r\n            real_features = np.array(real_output.features)\r\n            \r\n            cosine_sim = np.dot(sim_features, real_features) / (\r\n                np.linalg.norm(sim_features) * np.linalg.norm(real_features)\r\n            )\r\n            return cosine_sim\r\n        else:\r\n            # Fallback to comparing raw outputs if feature extraction isn't available\r\n            return np.mean(np.abs(sim_output - real_output))  # Mean absolute difference\r\n            \r\n    def evaluate_policy_in_sim(self, policy_model):\r\n        \"\"\"Evaluate policy in simulation environment\"\"\"\r\n        # This would involve running the policy in your simulation environment\r\n        # For this example, we'll use a placeholder implementation\r\n        return np.random.normal(100, 10)  # Placeholder return value\r\n    \r\n    def evaluate_policy_on_real(self, policy_model):\r\n        \"\"\"Evaluate policy on real robot\"\"\"\r\n        # This would involve deploying the policy to the real robot\r\n        # and measuring task performance\r\n        # For this example, we'll use a placeholder implementation\r\n        return np.random.normal(85, 15)  # Placeholder return value (typically lower than sim)\r\n    \r\n    def apply_domain_randomization(self):\r\n        \"\"\"Apply domain randomization to improve sim-to-real transfer\"\"\"\r\n        # Randomize physics parameters in simulation\r\n        physics_params = {\r\n            'gravity': np.random.uniform(9.0, 10.0),\r\n            'friction': np.random.uniform(0.1, 0.9),\r\n            'mass_variance': np.random.uniform(0.9, 1.1),\r\n            'sensor_noise': np.random.uniform(0.01, 0.05)\r\n        }\r\n        \r\n        # Apply parameters to simulation\r\n        self.sim_model.set_physics_parameters(physics_params)\r\n        \r\n        # Randomize visual appearance\r\n        visual_params = {\r\n            'lighting': np.random.uniform(0.5, 1.5),\r\n            'texture_randomization': np.random.choice([True, False]),\r\n            'color_variance': np.random.uniform(0.8, 1.2)\r\n        }\r\n        \r\n        self.sim_model.set_visual_parameters(visual_params)\r\n        \r\n        return physics_params, visual_params\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-911-deployment-considerations-",children:"\ud83d\ude9a 9.11 Deployment Considerations \ud83d\ude9a"}),"\n",(0,t.jsx)(n.h3,{id:"-9111-real-time-performance-optimization-",children:"\ud83d\udcc8 9.11.1 Real-time Performance Optimization \ud83d\udcc8"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import time\r\nimport threading\r\nfrom queue import Queue\r\n\r\nclass RealTimeRobotController:\r\n    def __init__(self, perception_model, policy_model, control_frequency=50):\r\n        self.perception_model = perception_model\r\n        self.policy_model = policy_model\r\n        self.control_frequency = control_frequency  # Hz\r\n        self.control_period = 1.0 / control_frequency\r\n        \r\n        # Threading for parallel processing\r\n        self.sensor_data_queue = Queue(maxsize=10)\r\n        self.action_queue = Queue(maxsize=1)\r\n        \r\n        # Performance monitoring\r\n        self.last_perception_time = 0\r\n        self.last_policy_time = 0\r\n        self.control_loop_time = 0\r\n        \r\n        # Threading control\r\n        self.running = False\r\n        self.perception_thread = None\r\n        self.policy_thread = None\r\n        \r\n    def preprocess_sensor_data(self, raw_sensors):\r\n        """Optimized preprocessing of sensor data"""\r\n        # Convert sensor data to appropriate format\r\n        processed_data = {}\r\n        \r\n        if \'camera\' in raw_sensors:\r\n            # Resize and normalize image data\r\n            img = raw_sensors[\'camera\']\r\n            processed_data[\'camera\'] = self.resize_and_normalize_image(img)\r\n        \r\n        if \'lidar\' in raw_sensors:\r\n            # Process LiDAR data efficiently\r\n            lidar = raw_sensors[\'lidar\']\r\n            processed_data[\'lidar\'] = self.process_lidar_data(lidar)\r\n            \r\n        if \'imu\' in raw_sensors:\r\n            # Process IMU data\r\n            imu = raw_sensors[\'imu\']\r\n            processed_data[\'imu\'] = self.process_imu_data(imu)\r\n        \r\n        return processed_data\r\n    \r\n    def resize_and_normalize_image(self, img):\r\n        """Efficient image preprocessing"""\r\n        # Resize using efficient method\r\n        resized = cv2.resize(img, (224, 224))  # Example size\r\n        # Normalize to [0, 1] and transpose to channel-first\r\n        normalized = (resized.astype(np.float32) / 255.0).transpose(2, 0, 1)\r\n        return normalized\r\n    \r\n    def process_lidar_data(self, lidar):\r\n        """Efficient LiDAR preprocessing"""\r\n        # Convert to tensor and normalize if needed\r\n        return torch.FloatTensor(lidar).unsqueeze(0)\r\n    \r\n    def process_imu_data(self, imu):\r\n        """Efficient IMU preprocessing"""\r\n        return torch.FloatTensor(imu).unsqueeze(0)\r\n    \r\n    def perception_worker(self):\r\n        """Worker thread for perception processing"""\r\n        while self.running:\r\n            try:\r\n                # Get raw sensor data\r\n                raw_data = self.sensor_data_queue.get(timeout=0.1)\r\n                \r\n                start_time = time.time()\r\n                \r\n                # Process perception\r\n                processed_data = self.preprocess_sensor_data(raw_data)\r\n                perception_output = self.perception_model(processed_data)\r\n                \r\n                self.last_perception_time = time.time() - start_time\r\n                \r\n                # Add to next processing queue\r\n                self.policy_queue.put(perception_output)\r\n                \r\n            except Exception as e:\r\n                print(f"Perception worker error: {e}")\r\n                \r\n    def policy_worker(self):\r\n        """Worker thread for policy decision"""\r\n        while self.running:\r\n            try:\r\n                # Get perception output\r\n                perception_data = self.policy_queue.get(timeout=0.1)\r\n                \r\n                start_time = time.time()\r\n                \r\n                # Get action from policy\r\n                action = self.policy_model(perception_data)\r\n                \r\n                self.last_policy_time = time.time() - start_time\r\n                \r\n                # Add to action queue\r\n                if not self.action_queue.full():\r\n                    self.action_queue.put(action)\r\n                \r\n            except Exception as e:\r\n                print(f"Policy worker error: {e}")\r\n    \r\n    def run_control_loop(self):\r\n        """Main real-time control loop"""\r\n        self.running = True\r\n        \r\n        # Start worker threads\r\n        self.perception_thread = threading.Thread(target=self.perception_worker)\r\n        self.policy_thread = threading.Thread(target=self.policy_worker)\r\n        \r\n        self.perception_thread.start()\r\n        self.policy_thread.start()\r\n        \r\n        try:\r\n            while self.running:\r\n                start_time = time.time()\r\n                \r\n                # Get latest action\r\n                if not self.action_queue.empty():\r\n                    action = self.action_queue.get()\r\n                    \r\n                    # Send action to robot\r\n                    self.send_action_to_robot(action)\r\n                \r\n                # Maintain control frequency\r\n                elapsed = time.time() - start_time\r\n                sleep_time = max(0, self.control_period - elapsed)\r\n                \r\n                if sleep_time > 0:\r\n                    time.sleep(sleep_time)\r\n                \r\n                self.control_loop_time = time.time() - start_time\r\n                \r\n        except KeyboardInterrupt:\r\n            print("Control loop interrupted")\r\n        finally:\r\n            self.stop()\r\n    \r\n    def send_action_to_robot(self, action):\r\n        """Send action to robot hardware"""\r\n        # Implementation depends on your robot interface\r\n        # This is a placeholder\r\n        pass\r\n    \r\n    def stop(self):\r\n        """Stop the controller"""\r\n        self.running = False\r\n        if self.perception_thread:\r\n            self.perception_thread.join()\r\n        if self.policy_thread:\r\n            self.policy_thread.join()\r\n\r\n# \u2699\ufe0f Additional optimization techniques \u2699\ufe0f\r\n\r\ndef optimize_model_for_inference(model, input_shape, quantize=False):\r\n    """Optimize a PyTorch model for inference"""\r\n    model.eval()\r\n    \r\n    # Trace the model\r\n    example_input = torch.randn(input_shape)\r\n    traced_model = torch.jit.trace(model, example_input)\r\n    \r\n    if quantize:\r\n        # Quantize the model for faster inference (reduces precision but increases speed)\r\n        quantized_model = torch.quantization.quantize_dynamic(\r\n            model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\r\n        )\r\n        return quantized_model\r\n    \r\n    return traced_model\r\n\r\ndef model_pruning(model, pruning_ratio=0.2):\r\n    """Prune a neural network model to reduce size and improve speed"""\r\n    import torch.nn.utils.prune as prune\r\n    \r\n    # Define parameters to prune\r\n    parameters_to_prune = []\r\n    for name, module in model.named_modules():\r\n        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\r\n            parameters_to_prune.append((module, "weight"))\r\n    \r\n    # Apply pruning\r\n    for module, param_name in parameters_to_prune:\r\n        prune.l1_unstructured(module, name=param_name, amount=pruning_ratio)\r\n    \r\n    # Remove pruning reparameterization so it\'s permanent\r\n    for name, module in model.named_modules():\r\n        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\r\n            prune.remove(module, "weight")\r\n    \r\n    return model\n'})}),"\n",(0,t.jsx)(n.h2,{id:"-912-summary-",children:"\ud83d\udcdd 9.12 Summary \ud83d\udcdd"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covered key aspects of robot learning and AI integration in physical AI systems. We explored:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Systems"}),": Deep learning approaches for vision, segmentation, and depth estimation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robot Control"}),": Supervised learning, PID controllers enhanced with ML, and sequential decision making"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deep Learning Architectures"}),": RNNs, attention mechanisms, and ConvLSTMs for robotics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reinforcement Learning"}),": DQN, PPO, and SAC algorithms for robot control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Fusion"}),": Kalman filters, particle filters, and neural fusion methods"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning from Demonstrations"}),": Behavior cloning, DAgger, and GAIL approaches"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation & Path Planning"}),": Learning-enhanced A* and RRT* algorithms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human-Robot Interaction Learning"}),": Learning from human feedback and interactive learning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluation and Validation"}),": Metrics for robot learning and sim-to-real transfer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deployment Considerations"}),": Real-time performance optimization"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The implementations in this chapter provide foundational tools for developing learning-enabled robots. These techniques enable robots to adapt to new situations, learn complex behaviors, and improve performance over time."}),"\n",(0,t.jsx)(n.h3,{id:"\u2139\ufe0f-key-takeaways-\u2139\ufe0f",children:"\u2139\ufe0f Key Takeaways: \u2139\ufe0f"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot learning requires special consideration for safety, sample efficiency, and real-time performance"}),"\n",(0,t.jsx)(n.li,{children:"Deep learning architectures can be adapted for robotics-specific tasks like perception and control"}),"\n",(0,t.jsx)(n.li,{children:"Reinforcement learning offers powerful methods for learning complex behaviors through environment interaction"}),"\n",(0,t.jsx)(n.li,{children:"Proper evaluation and validation are crucial for deploying learning systems on physical robots"}),"\n",(0,t.jsx)(n.li,{children:"Simulation-to-real transfer remains challenging but can be improved with domain randomization and careful validation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"-knowledge-check-",children:"\ud83e\udd14 Knowledge Check \ud83e\udd14"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Explain the differences between behavior cloning, DAgger, and GAIL in imitation learning."}),"\n",(0,t.jsx)(n.li,{children:"Compare the advantages and disadvantages of DQN vs. PPO vs. SAC for robotic control."}),"\n",(0,t.jsx)(n.li,{children:"Describe how sensor fusion can improve robot perception and decision-making."}),"\n",(0,t.jsx)(n.li,{children:"What are the key challenges in deploying machine learning models on physical robots?"}),"\n",(0,t.jsx)(n.li,{children:"How can domain randomization improve sim-to-real transfer?"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.em,{children:["Continue to ",(0,t.jsx)(n.a,{href:"/Humanoid-Robotic-Book/docs/part-4-ai-brain/chapter-10-reinforcement-learning-control",children:"Chapter 10: Vision-Language-Action Integration"})]})})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var t=r(6540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);